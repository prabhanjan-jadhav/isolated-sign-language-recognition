{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0266692f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:48.822663Z",
     "iopub.status.busy": "2023-03-24T10:59:48.822196Z",
     "iopub.status.idle": "2023-03-24T10:59:51.566109Z",
     "shell.execute_reply": "2023-03-24T10:59:51.564688Z"
    },
    "papermill": {
     "duration": 2.762081,
     "end_time": "2023-03-24T10:59:51.569492",
     "exception": false,
     "start_time": "2023-03-24T10:59:48.807411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b95d051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:51.594529Z",
     "iopub.status.busy": "2023-03-24T10:59:51.592936Z",
     "iopub.status.idle": "2023-03-24T10:59:51.599787Z",
     "shell.execute_reply": "2023-03-24T10:59:51.598610Z"
    },
    "papermill": {
     "duration": 0.022174,
     "end_time": "2023-03-24T10:59:51.602727",
     "exception": false,
     "start_time": "2023-03-24T10:59:51.580553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 80\n",
    "num_point  = 82\n",
    "\n",
    "embed_dim  = 512\n",
    "num_head   = 4\n",
    "num_block  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee4a695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:51.626847Z",
     "iopub.status.busy": "2023-03-24T10:59:51.626352Z",
     "iopub.status.idle": "2023-03-24T10:59:52.105454Z",
     "shell.execute_reply": "2023-03-24T10:59:52.104289Z"
    },
    "papermill": {
     "duration": 0.499135,
     "end_time": "2023-03-24T10:59:52.112815",
     "exception": false,
     "start_time": "2023-03-24T10:59:51.613680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "                           label : torch.Size([4]) \n",
      "                             xyz : torch.Size([12, 82, 3]) \n",
      "                                 : torch.Size([16, 82, 3]) \n",
      "                                 : torch.Size([20, 82, 3]) \n",
      "                                 : torch.Size([180, 82, 3]) \n",
      "output\n",
      "                            sign : torch.Size([4, 250]) \n",
      "loss\n",
      "                      label_loss : 5.658082962036133 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_class  = 250\n",
    "num_landmark = 543\n",
    "\n",
    "class HardSwish(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x+3) * 0.16666667\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "#https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            num_head,\n",
    "            batch_first,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads=num_head,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            kdim=None,\n",
    "            vdim=None,\n",
    "            dropout=0.0,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        out, _ = self.mha(x,x,x, key_padding_mask=x_mask)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        embed_dim,\n",
    "        num_head,\n",
    "        out_dim,\n",
    "        batch_first=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn  = MultiHeadAttention(embed_dim, num_head,batch_first)\n",
    "        self.ffn   = FeedForward(embed_dim, out_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        x = x + self.attn((self.norm1(x)), x_mask)\n",
    "        x = x + self.ffn((self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "def positional_encoding(length, embed_dim):\n",
    "    dim = embed_dim//2\n",
    "    position = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    dim = np.arange(dim)[np.newaxis, :]/dim   # (1, dim)\n",
    "    angle = 1 / (10000**dim)         # (1, dim)\n",
    "    angle = position * angle    # (pos, dim)\n",
    "    pos_embed = np.concatenate(\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "        axis=-1\n",
    "    )\n",
    "    pos_embed = torch.from_numpy(pos_embed).float()\n",
    "    return pos_embed\n",
    "\n",
    "def pack_seq(\n",
    "    seq,\n",
    "):\n",
    "    length = [min(s.shape[0], max_length)  for s in seq]\n",
    "    batch_size = len(seq)\n",
    "    K = seq[0].shape[1]\n",
    "    L = max(length)\n",
    "\n",
    "    x = torch.zeros((batch_size, L, K, 3)).to(seq[0].device)\n",
    "    x_mask = torch.zeros((batch_size, L)).to(seq[0].device)\n",
    "    for b in range(batch_size):\n",
    "        l = length[b]\n",
    "        x[b, :l] = seq[b][:l]\n",
    "        x_mask[b, l:] = 1\n",
    "    x_mask = (x_mask>0.5)\n",
    "    x = x.reshape(batch_size,-1,K*3)\n",
    "    return x, x_mask\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=num_class):\n",
    "        super().__init__()\n",
    "        self.output_type = ['inference', 'loss']\n",
    "\n",
    "        pos_embed = positional_encoding(max_length, embed_dim)\n",
    "        # self.register_buffer('pos_embed', pos_embed)\n",
    "        self.pos_embed = nn.Parameter(pos_embed)\n",
    "\n",
    "        self.cls_embed = nn.Parameter(torch.zeros((1, embed_dim)))\n",
    "        self.x_embed = nn.Sequential(\n",
    "            nn.Linear(num_point * 3, embed_dim, bias=False),\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_head,\n",
    "                embed_dim,\n",
    "            ) for i in range(num_block)\n",
    "        ])\n",
    "        self.logit = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        xyz = batch['xyz']\n",
    "        x, x_mask = pack_seq(xyz)\n",
    "        #print(x.shape, x_mask.shape)\n",
    "        B,L,_ = x.shape\n",
    "        x = self.x_embed(x)\n",
    "        x = x + self.pos_embed[:L].unsqueeze(0)\n",
    "\n",
    "        x = torch.cat([\n",
    "            self.cls_embed.unsqueeze(0).repeat(B,1,1),\n",
    "            x\n",
    "        ],1)\n",
    "        x_mask = torch.cat([\n",
    "            torch.zeros(B,1).to(x_mask),\n",
    "            x_mask\n",
    "        ],1)\n",
    "\n",
    "\n",
    "        #x = F.dropout(x,p=0.25,training=self.training)\n",
    "        for block in self.encoder:\n",
    "            x = block(x,x_mask)\n",
    "\n",
    "        cls = x[:,0]\n",
    "        cls = F.dropout(cls,p=0.4,training=self.training)\n",
    "        logit = self.logit(cls)\n",
    "\n",
    "        output = {}\n",
    "        if 'loss' in self.output_type:\n",
    "            output['label_loss'] = F.cross_entropy(logit, batch['label'])\n",
    "\n",
    "        if 'inference' in self.output_type:\n",
    "            output['sign'] = torch.softmax(logit,-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_check_net():\n",
    "\n",
    "    length = [12,16,20,180]\n",
    "    batch_size = len(length)\n",
    "    xyz = [\n",
    "        np.random.uniform(-1,1,(length[b],num_point,3)) for b in range(batch_size)\n",
    "    ]\n",
    "    #---\n",
    "    batch = {\n",
    "        'label' : torch.from_numpy( np.random.choice(250,(batch_size))).long(),\n",
    "        'xyz' : [torch.from_numpy(x).float() for x in xyz]\n",
    "    }\n",
    "\n",
    "    net = Net()\n",
    "    output = net(batch)\n",
    "\n",
    "\n",
    "    #---\n",
    "\n",
    "    print('batch')\n",
    "    for k, v in batch.items():\n",
    "        if k in ['label','x']:\n",
    "            print(f'{k:>32} : {v.shape} ')\n",
    "        if k=='xyz':\n",
    "            print(f'{k:>32} : {v[0].shape} ')\n",
    "            for i in range(1,len(v)):\n",
    "                print(f'{\" \":>32} : {v[i].shape} ')\n",
    "\n",
    "    print('output')\n",
    "    for k, v in output.items():\n",
    "        if 'loss' not in k:\n",
    "            print(f'{k:>32} : {v.shape} ')\n",
    "    print('loss')\n",
    "    for k, v in output.items():\n",
    "        if 'loss' in k:\n",
    "            print(f'{k:>32} : {v.item()} ')\n",
    "\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "    run_check_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b2bf3",
   "metadata": {
    "papermill": {
     "duration": 0.010816,
     "end_time": "2023-03-24T10:59:52.138460",
     "exception": false,
     "start_time": "2023-03-24T10:59:52.127644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54fc9ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:52.163327Z",
     "iopub.status.busy": "2023-03-24T10:59:52.162755Z",
     "iopub.status.idle": "2023-03-24T10:59:52.170255Z",
     "shell.execute_reply": "2023-03-24T10:59:52.168913Z"
    },
    "papermill": {
     "duration": 0.023517,
     "end_time": "2023-03-24T10:59:52.173257",
     "exception": false,
     "start_time": "2023-03-24T10:59:52.149740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# additional helper functions\n",
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d578d6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:52.198105Z",
     "iopub.status.busy": "2023-03-24T10:59:52.197556Z",
     "iopub.status.idle": "2023-03-24T10:59:55.781738Z",
     "shell.execute_reply": "2023-03-24T10:59:55.780384Z"
    },
    "papermill": {
     "duration": 3.600487,
     "end_time": "2023-03-24T10:59:55.784954",
     "exception": false,
     "start_time": "2023-03-24T10:59:52.184467",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlen = 22959\n",
      "\tnum_participant_id = 5\n",
      "\n",
      "0 --------------------\n",
      "path                      train_landmark_files/49445/1000397667.parquet\n",
      "participant_id                                                    49445\n",
      "sequence_id                                                  1000397667\n",
      "sign                                                             vacuum\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                               231\n",
      "fold                                                                  0\n",
      "Name: 0, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([33, 82, 3])\n",
      "\t min/max: -1.4137358665466309 / 1.5335360765457153\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.4290144741535187, 0.43524935841560364, -0.8092206120491028, 0.4351654648780823, 0.4245232045650482] ...\n",
      "\t\t [1.5183755159378052, -0.9306944608688354, 0.30079182982444763, 1.5102177858352661, -0.8800152540206909]\n",
      "\n",
      "1 --------------------\n",
      "path                      train_landmark_files/61333/1000909322.parquet\n",
      "participant_id                                                    61333\n",
      "sequence_id                                                  1000909322\n",
      "sign                                                              shirt\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                               195\n",
      "fold                                                                  0\n",
      "Name: 1, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([22, 82, 3])\n",
      "\t min/max: -1.6253658533096313 / 2.0139806270599365\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.1677067130804062, 0.7675708532333374, -0.8798531293869019, 0.16568496823310852, 0.7492659091949463] ...\n",
      "\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "2 --------------------\n",
      "path                       train_landmark_files/4718/1001385785.parquet\n",
      "participant_id                                                     4718\n",
      "sequence_id                                                  1001385785\n",
      "sign                                                              clown\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                49\n",
      "fold                                                                  0\n",
      "Name: 2, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([80, 82, 3])\n",
      "\t min/max: -1.2869410514831543 / 1.0447239875793457\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.34457674622535706, 0.5869734883308411, -0.9500903487205505, 0.3376515805721283, 0.5622150301933289] ...\n",
      "\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "3 --------------------\n",
      "path                      train_landmark_files/49445/1001499433.parquet\n",
      "participant_id                                                    49445\n",
      "sequence_id                                                  1001499433\n",
      "sign                                                              store\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                               207\n",
      "fold                                                                  0\n",
      "Name: 3, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([6, 82, 3])\n",
      "\t min/max: -0.9011333584785461 / 0.7579073309898376\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.4424348771572113, 0.3435496389865875, -0.7855185270309448, 0.45140770077705383, 0.33394286036491394] ...\n",
      "\t\t [0.3738210201263428, -0.8777258992195129, -0.25246328115463257, 0.4281977713108063, -0.8668199181556702]\n",
      "\n",
      "4 --------------------\n",
      "path                      train_landmark_files/61333/1001819372.parquet\n",
      "participant_id                                                    61333\n",
      "sequence_id                                                  1001819372\n",
      "sign                                                            balloon\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                14\n",
      "fold                                                                  0\n",
      "Name: 4, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([27, 82, 3])\n",
      "\t min/max: -1.4120255708694458 / 1.9696542024612427\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.348316490650177, 0.5634872913360596, -1.008692979812622, 0.36670276522636414, 0.5481652021408081] ...\n",
      "\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "5 --------------------\n",
      "path                       train_landmark_files/2044/1001950812.parquet\n",
      "participant_id                                                     2044\n",
      "sequence_id                                                  1001950812\n",
      "sign                                                               milk\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                               142\n",
      "fold                                                                  0\n",
      "Name: 5, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([27, 82, 3])\n",
      "\t min/max: -1.4269397258758545 / 1.64667809009552\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.35638466477394104, 0.5107186436653137, -0.9226750135421753, 0.36864763498306274, 0.5006067156791687] ...\n",
      "\t\t [1.486141324043274, -1.1071124076843262, -0.10997430980205536, 1.522465467453003, -1.0708894729614258]\n",
      "\n",
      "6 --------------------\n",
      "path                      train_landmark_files/49445/1002020383.parquet\n",
      "participant_id                                                    49445\n",
      "sequence_id                                                  1002020383\n",
      "sign                                                             drawer\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                62\n",
      "fold                                                                  0\n",
      "Name: 6, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([60, 82, 3])\n",
      "\t min/max: -1.342134714126587 / 1.406419277191162\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.6350717544555664, 0.24549315869808197, -0.7391493320465088, 0.6425521969795227, 0.23781165480613708] ...\n",
      "\t\t [1.3408353328704834, -0.7479751706123352, 0.5829577445983887, 1.3471013307571411, -0.7190732359886169]\n",
      "\n",
      "7 --------------------\n",
      "path                      train_landmark_files/61333/1002052130.parquet\n",
      "participant_id                                                    61333\n",
      "sequence_id                                                  1002052130\n",
      "sign                                                                 TV\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                 0\n",
      "fold                                                                  0\n",
      "Name: 7, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([80, 82, 3])\n",
      "\t min/max: -1.5962445735931396 / 2.010420322418213\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.16069254279136658, 0.6612129807472229, -1.006607174873352, 0.17581970989704132, 0.6479062438011169] ...\n",
      "\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "8 --------------------\n",
      "path                       train_landmark_files/2044/1002091184.parquet\n",
      "participant_id                                                     2044\n",
      "sequence_id                                                  1002091184\n",
      "sign                                                               duck\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                67\n",
      "fold                                                                  0\n",
      "Name: 8, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([18, 82, 3])\n",
      "\t min/max: -1.3234901428222656 / 1.0862078666687012\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.3130979835987091, 0.5779224634170532, -0.8698641061782837, 0.3187340795993805, 0.563902735710144] ...\n",
      "\t\t [0.9609947800636292, -1.1268714666366577, -0.0030901161953806877, 0.9988311529159546, -1.0945731401443481]\n",
      "\n",
      "9 --------------------\n",
      "path                       train_landmark_files/2044/1002092995.parquet\n",
      "participant_id                                                     2044\n",
      "sequence_id                                                  1002092995\n",
      "sign                                                               blow\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                25\n",
      "fold                                                                  0\n",
      "Name: 9, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([6, 82, 3])\n",
      "\t min/max: -1.0173239707946777 / 1.2186691761016846\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.24501162767410278, 0.6447877883911133, -0.8113453984260559, 0.2502824366092682, 0.6313385963439941] ...\n",
      "\t\t [0.7517496943473816, -1.0173239707946777, 0.4567146301269531, 0.770895779132843, -1.0159655809402466]\n",
      "\n",
      "10 --------------------\n",
      "path                      train_landmark_files/37779/1002526690.parquet\n",
      "participant_id                                                    37779\n",
      "sequence_id                                                  1002526690\n",
      "sign                                                                bee\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                                19\n",
      "fold                                                                  0\n",
      "Name: 10, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([7, 82, 3])\n",
      "\t min/max: -1.0755250453948975 / 1.4256017208099365\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.3668118119239807, 0.6079865097999573, -0.8985936045646667, 0.38047438859939575, 0.6029800176620483] ...\n",
      "\t\t [0.6797853112220764, -1.0417932271957397, -0.13120238482952118, 0.6129040122032166, -1.0502601861953735]\n",
      "\n",
      "11 --------------------\n",
      "path                      train_landmark_files/37779/1002801453.parquet\n",
      "participant_id                                                    37779\n",
      "sequence_id                                                  1002801453\n",
      "sign                                                           yourself\n",
      "landmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\n",
      "npy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\n",
      "label                                                               246\n",
      "fold                                                                  0\n",
      "Name: 11, dtype: object \n",
      "\n",
      "xyz\n",
      "\t dtype: torch.float32\n",
      "\t shape: torch.Size([7, 82, 3])\n",
      "\t min/max: -2.361575126647949 / 1.7345937490463257\n",
      "\t is_contiguous: True\n",
      "\t values:\n",
      "\t\t [0.37138479948043823, 0.5532127618789673, -0.888463020324707, 0.38432031869888306, 0.5442971587181091] ...\n",
      "\t\t [1.7210133075714111, -1.8778843879699707, -0.20284615457057953, 1.7345937490463257, -1.9374130964279175]\n",
      "\n",
      "batch_size   : 8\n",
      "len(loader)  : 2869\n",
      "len(dataset) : 22959\n",
      "\n",
      "batch  0 ===================\n",
      "index [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "xyz:\n",
      "\t torch.Size([33, 82, 3])\n",
      "\t torch.Size([22, 82, 3])\n",
      "\t torch.Size([80, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([27, 82, 3])\n",
      "\t torch.Size([27, 82, 3])\n",
      "\t torch.Size([60, 82, 3])\n",
      "\t torch.Size([80, 82, 3])\n",
      "label:\n",
      "\t [231, 195, 49, 207, 14, 142, 62, 0]\n",
      "\n",
      "batch  1 ===================\n",
      "index [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "xyz:\n",
      "\t torch.Size([18, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([7, 82, 3])\n",
      "\t torch.Size([7, 82, 3])\n",
      "\t torch.Size([23, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([19, 82, 3])\n",
      "\t torch.Size([21, 82, 3])\n",
      "label:\n",
      "\t [67, 25, 19, 246, 172, 206, 243, 51]\n",
      "\n",
      "batch  2 ===================\n",
      "index [16, 17, 18, 19, 20, 21, 22, 23]\n",
      "xyz:\n",
      "\t torch.Size([13, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([53, 82, 3])\n",
      "\t torch.Size([10, 82, 3])\n",
      "\t torch.Size([24, 82, 3])\n",
      "\t torch.Size([10, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([55, 82, 3])\n",
      "label:\n",
      "\t [247, 147, 239, 217, 136, 138, 17, 85]\n",
      "\n",
      "batch  3 ===================\n",
      "index [24, 25, 26, 27, 28, 29, 30, 31]\n",
      "xyz:\n",
      "\t torch.Size([10, 82, 3])\n",
      "\t torch.Size([8, 82, 3])\n",
      "\t torch.Size([9, 82, 3])\n",
      "\t torch.Size([12, 82, 3])\n",
      "\t torch.Size([26, 82, 3])\n",
      "\t torch.Size([3, 82, 3])\n",
      "\t torch.Size([80, 82, 3])\n",
      "\t torch.Size([11, 82, 3])\n",
      "label:\n",
      "\t [115, 220, 61, 209, 110, 73, 45, 90]\n",
      "\n",
      "batch  4 ===================\n",
      "index [32, 33, 34, 35, 36, 37, 38, 39]\n",
      "xyz:\n",
      "\t torch.Size([45, 82, 3])\n",
      "\t torch.Size([14, 82, 3])\n",
      "\t torch.Size([3, 82, 3])\n",
      "\t torch.Size([24, 82, 3])\n",
      "\t torch.Size([13, 82, 3])\n",
      "\t torch.Size([45, 82, 3])\n",
      "\t torch.Size([8, 82, 3])\n",
      "\t torch.Size([25, 82, 3])\n",
      "label:\n",
      "\t [165, 23, 238, 117, 195, 67, 223, 4]\n",
      "\n",
      "batch  5 ===================\n",
      "index [40, 41, 42, 43, 44, 45, 46, 47]\n",
      "xyz:\n",
      "\t torch.Size([8, 82, 3])\n",
      "\t torch.Size([48, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([6, 82, 3])\n",
      "\t torch.Size([9, 82, 3])\n",
      "\t torch.Size([80, 82, 3])\n",
      "\t torch.Size([34, 82, 3])\n",
      "\t torch.Size([18, 82, 3])\n",
      "label:\n",
      "\t [52, 75, 77, 93, 200, 226, 60, 65]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import SequentialSampler, RandomSampler\n",
    "\n",
    "def read_kaggle_csv_by_random(fold=0):\n",
    "    num_fold = 5\n",
    "\n",
    "    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n",
    "    train_df = kaggle_df[kaggle_df.fold!=fold].reset_index(drop=True)\n",
    "    valid_df = kaggle_df[kaggle_df.fold==fold].reset_index(drop=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "def read_kaggle_csv_by_part(fold=0):\n",
    "    num_fold = 5\n",
    "\n",
    "    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n",
    "    kaggle_df.loc[:, 'fold' ] = -1\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=num_fold, random_state=123, shuffle=True)\n",
    "    for i, (train_index, valid_index) in enumerate(sgkf.split(kaggle_df.path, kaggle_df.label, kaggle_df.participant_id)):\n",
    "        kaggle_df.loc[valid_index,'fold'] = i\n",
    "\n",
    "    #kaggle_df.loc[:, 'fold'] = np.arange(len(kaggle_df))%num_fold\n",
    "    train_df = kaggle_df[kaggle_df.fold!=fold].reset_index(drop=True)\n",
    "    valid_df = kaggle_df[kaggle_df.fold==fold].reset_index(drop=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "def read_christ_csv_by_part(fold=0):\n",
    "    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n",
    "    christ_df = kaggle_df\n",
    "    \n",
    "    christ_df = christ_df.merge(kaggle_df[['path']], on='path',validate='1:1') # also kaggle_df['num_frame'] was there removed it\n",
    "    valid_df = christ_df[christ_df.fold == fold].reset_index(drop=True)\n",
    "    train_df = christ_df[christ_df.fold != fold].reset_index(drop=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def pre_process(xyz):\n",
    "    #xyz = xyz - xyz[~torch.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n",
    "    #xyz = xyz / xyz[~torch.isnan(xyz)].std(0, keepdims=True)\n",
    "    \n",
    "    LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "    \n",
    "    lip   = xyz[:, LIP]\n",
    "    lhand = xyz[:, 468:489]\n",
    "    rhand = xyz[:, 522:543]\n",
    "    xyz = torch.cat([ #(none, 82, 3)\n",
    "        lip,\n",
    "        lhand,\n",
    "        rhand,\n",
    "    ],1)\n",
    "    xyz[torch.isnan(xyz)] = 0\n",
    "    xyz = xyz[:max_length]\n",
    "    return xyz\n",
    "\n",
    "\n",
    "#-----------------------------------------------------\n",
    "def train_augment(xyz):\n",
    "    xyz = do_random_affine(\n",
    "        xyz,\n",
    "        scale  = (0.7,1.3),\n",
    "        shift  = (-0.08,0.08),\n",
    "        degree = (-20,20),\n",
    "        p=0.8\n",
    "    )\n",
    "    return xyz\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        num_participant_id = self.df.participant_id.nunique()\n",
    "        string = ''\n",
    "        string += f'\\tlen = {len(self)}\\n'\n",
    "        string += f'\\tnum_participant_id = {num_participant_id}\\n'\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "\n",
    "        pq_file = f'/kaggle/input/asl-signs/{d.path}'\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "#         print(xyz)\n",
    "        xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n",
    "        xyz = xyz / xyz[~np.isnan(xyz)].std(0, keepdims=True)\n",
    "\n",
    "        #--\n",
    "#         if self.augment is not None:\n",
    "#             xyz = self.augment(xyz)\n",
    "#         print(xyz)\n",
    "        #--\n",
    "        xyz = torch.from_numpy(xyz).float()\n",
    "        xyz = pre_process(xyz)\n",
    "\n",
    "        r = {}\n",
    "        r['index'] = index\n",
    "        r['d'    ] = d\n",
    "        r['xyz'  ] = xyz\n",
    "        r['label'] = d.label\n",
    "        return r\n",
    "\n",
    "\n",
    "tensor_key = ['xyz', 'label']\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        d[k] = [b[k] for b in batch]\n",
    "    d['label'] = torch.LongTensor(d['label'])\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def run_check_dataset():\n",
    "\n",
    "    train_df, valid_df = read_kaggle_csv_by_part(fold=0)\n",
    "    dataset = SignDataset(valid_df)\n",
    "    print(dataset)\n",
    "\n",
    "    for i in range(12):\n",
    "        r = dataset[i]\n",
    "        print(r['index'], '--------------------')\n",
    "        print(r[\"d\"], '\\n')\n",
    "        for k in tensor_key:\n",
    "            if k =='label': continue\n",
    "            v = r[k]\n",
    "            print(k)\n",
    "            print('\\t', 'dtype:', v.dtype)\n",
    "            print('\\t', 'shape:', v.shape)\n",
    "            if len(v)!=0:\n",
    "                print('\\t', 'min/max:', v.min().item(),'/', v.max().item())\n",
    "                print('\\t', 'is_contiguous:', v.is_contiguous())\n",
    "                print('\\t', 'values:')\n",
    "                print('\\t\\t', v.reshape(-1)[:5].data.numpy().tolist(), '...')\n",
    "                print('\\t\\t', v.reshape(-1)[-5:].data.numpy().tolist())\n",
    "        print('')\n",
    "        if 0:\n",
    "            #draw\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=SequentialSampler(dataset),\n",
    "        batch_size=8,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "        collate_fn=null_collate,\n",
    "    )\n",
    "    print(f'batch_size   : {loader.batch_size}')\n",
    "    print(f'len(loader)  : {len(loader)}')\n",
    "    print(f'len(dataset) : {len(dataset)}')\n",
    "    print('')\n",
    "\n",
    "    for t, batch in enumerate(loader):\n",
    "        if t > 5: break\n",
    "        print('batch ', t, '===================')\n",
    "        print('index', batch['index'])\n",
    "\n",
    "        for k in tensor_key:\n",
    "            v = batch[k]\n",
    "\n",
    "            if k =='label':\n",
    "                print('label:')\n",
    "                print('\\t', v.data.numpy().tolist())\n",
    "\n",
    "            if k =='x':\n",
    "                print('x:')\n",
    "                print('\\t', v.data.shape)\n",
    "\n",
    "            if k =='xyz':\n",
    "                print('xyz:')\n",
    "                for i in range(len(v)):\n",
    "                    print('\\t', v[i].shape)\n",
    "\n",
    "        if 1:\n",
    "            pass\n",
    "        print('')\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "    run_check_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0c1a7",
   "metadata": {
    "papermill": {
     "duration": 0.010855,
     "end_time": "2023-03-24T10:59:55.809400",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.798545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78083b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:55.834437Z",
     "iopub.status.busy": "2023-03-24T10:59:55.833947Z",
     "iopub.status.idle": "2023-03-24T10:59:55.839192Z",
     "shell.execute_reply": "2023-03-24T10:59:55.837925Z"
    },
    "papermill": {
     "duration": 0.021531,
     "end_time": "2023-03-24T10:59:55.842115",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.820584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6238109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:55.869740Z",
     "iopub.status.busy": "2023-03-24T10:59:55.868969Z",
     "iopub.status.idle": "2023-03-24T10:59:55.881518Z",
     "shell.execute_reply": "2023-03-24T10:59:55.880108Z"
    },
    "papermill": {
     "duration": 0.029021,
     "end_time": "2023-03-24T10:59:55.884368",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.855347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# additional helper functions 2\n",
    "#assum zero-mean one-std, input\n",
    "def do_random_affine(xyz,\n",
    "    scale  = (0.8,1.5),\n",
    "    shift  = (-0.1,0.1),\n",
    "    degree = (-15,15),\n",
    "    p=0.5\n",
    "):\n",
    "    if np.random.rand()<p:\n",
    "        if scale is not None:\n",
    "            scale = np.random.uniform(*scale)\n",
    "            xyz = scale*xyz\n",
    "\n",
    "        if shift is not None:\n",
    "            shift = np.random.uniform(*shift)\n",
    "            xyz = xyz + shift\n",
    "\n",
    "        if degree is not None:\n",
    "            degree = np.random.uniform(*degree)\n",
    "            radian = degree/180*np.pi\n",
    "            c = np.cos(radian)\n",
    "            s = np.sin(radian)\n",
    "            rotate = np.array([\n",
    "                [c,-s],\n",
    "                [s, c],\n",
    "            ]).T\n",
    "            xyz[...,:2] = xyz[...,:2] @rotate\n",
    "            \n",
    "def get_learning_rate(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c03bde",
   "metadata": {
    "papermill": {
     "duration": 0.010917,
     "end_time": "2023-03-24T10:59:55.906890",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.895973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RAdam and Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ecfd76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:55.932633Z",
     "iopub.status.busy": "2023-03-24T10:59:55.932154Z",
     "iopub.status.idle": "2023-03-24T10:59:55.944520Z",
     "shell.execute_reply": "2023-03-24T10:59:55.943201Z"
    },
    "papermill": {
     "duration": 0.028878,
     "end_time": "2023-03-24T10:59:55.947284",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.918406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# # class RAdam(Optimizer):\n",
    "# #     r\"\"\"Implements RAdam algorithm.\n",
    "# #     It has been proposed in `ON THE VARIANCE OF THE ADAPTIVE LEARNING\n",
    "# #     RATE AND BEYOND(https://arxiv.org/pdf/1908.03265.pdf)`_.\n",
    "    \n",
    "# #     Arguments:\n",
    "# #         params (iterable):      iterable of parameters to optimize or dicts defining\n",
    "# #                                 parameter groups\n",
    "# #         lr (float, optional):   learning rate (default: 1e-3)\n",
    "# #         betas (Tuple[float, float], optional):  coefficients used for computing\n",
    "# #                                                 running averages of gradient and \n",
    "# #                                                 its square (default: (0.9, 0.999))\n",
    "# #         eps (float, optional):  term added to the denominator to improve\n",
    "# #                                 numerical stability (default: 1e-8)\n",
    "# #         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "# #         amsgrad (boolean, optional):    whether to use the AMSGrad variant of this\n",
    "# #                                         algorithm from the paper `On the Convergence \n",
    "# #                                         of Adam and Beyond`_(default: False)\n",
    "        \n",
    "# #         sma_thresh:             simple moving average threshold.\n",
    "# #                                 Length till where the variance of adaptive lr is intracable.\n",
    "# #                                 Default: 4 (as per paper)\n",
    "# #     \"\"\"\n",
    "# #     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "# #                  weight_decay=0, amsgrad=False, sma_thresh=4):\n",
    "# #         if not 0.0 <= lr:\n",
    "# #             raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "# #         if not 0.0 <= eps:\n",
    "# #             raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "# #         if not 0.0 <= betas[0] < 1.0:\n",
    "# #             raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "# #         if not 0.0 <= betas[1] < 1.0:\n",
    "# #             raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "# #         defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "# #                         weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "# #         super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "# #         self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "# #         self.sma_thresh = sma_thresh\n",
    "\n",
    "# #     def __setstate__(self, state):\n",
    "# #         super(RAdam, self).__setstate__(state)\n",
    "# #         for group in self.param_groups:\n",
    "# #             group.setdefault('amsgrad', False)\n",
    "\n",
    "# #     def step(self, closure=None):\n",
    "# #         loss = None\n",
    "# #         if closure is not None:\n",
    "# #             loss = closure()\n",
    "\n",
    "# #         for group in self.param_groups:\n",
    "# #             for p in group['params']:\n",
    "# #                 if p.grad is None:\n",
    "# #                     continue\n",
    "\n",
    "# #                 # Perform optimization step\n",
    "# #                 grad = p.grad.data\n",
    "# #                 if grad.is_sparse:\n",
    "# #                     raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "# #                 amsgrad = group['amsgrad']\n",
    "\n",
    "# #                 state = self.state[p]\n",
    "\n",
    "# #                 # State initialization\n",
    "# #                 if len(state) == 0:\n",
    "# #                     state['step'] = 0\n",
    "# #                     # Exponential moving average of gradient values\n",
    "# #                     state['exp_avg'] = torch.zeros_like(p.data)\n",
    "# #                     # Exponential moving average of squared gradient values\n",
    "# #                     state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "# #                     if amsgrad:\n",
    "# #                         # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "# #                         state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "# #                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "# #                 if amsgrad:\n",
    "# #                     max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "# #                 beta1, beta2 = group['betas']\n",
    "\n",
    "# #                 state['step'] += 1\n",
    "# #                 old = p.data.float()\n",
    "\n",
    "# #                 # Decay the first and second moment running average coefficient\n",
    "# #                 exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "# #                 exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "# #                 buffer = self.radam_buffer[int(state['step']%10)]\n",
    "\n",
    "# #                 if buffer[0] == state['step']:\n",
    "# #                     sma_t, step_size = buffer[1], buffer[2]\n",
    "# #                 else:                 \n",
    "# #                     sma_max_len = 2/(1-beta2) - 1  \n",
    "# #                     beta2_t = beta2 ** state['step']\n",
    "# #                     sma_t = sma_max_len - 2 * state['step'] * beta2_t /(1 - beta2_t)\n",
    "# #                     buffer[0] = state['step']\n",
    "# #                     buffer[1] = sma_t\n",
    "\n",
    "# #                     if sma_t > self.sma_thresh :\n",
    "# #                         rt = math.sqrt(((sma_t - 4) * (sma_t - 2) * sma_max_len)/((sma_max_len -4) * (sma_max_len - 2) * sma_t))\n",
    "# #                         step_size = group['lr'] * rt * math.sqrt((1 - beta2_t)) / (1 - beta1 ** state['step'])                      \n",
    "# #                     else:\n",
    "# #                         step_size = group['lr'] / (1 - beta1 ** state['step'])                        \n",
    "# #                     buffer[2] = step_size\n",
    "\n",
    "# #                 if group['weight_decay'] != 0:\n",
    "# #                     p.data.add_(-group['weight_decay'] * group['lr'], old)\n",
    "\n",
    "# #                 if sma_t > self.sma_thresh :\n",
    "# #                     denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "# #                     p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "# #                 else:\n",
    "# #                     p.data.add_(-step_size, exp_avg)\n",
    "\n",
    "# #         return loss\n",
    "\n",
    "# class Lookahead(Optimizer):\n",
    "#     r'''Implements Lookahead optimizer.\n",
    "\n",
    "#     It's been proposed in paper: Lookahead Optimizer: k steps forward, 1 step back\n",
    "#     (https://arxiv.org/pdf/1907.08610.pdf)\n",
    "\n",
    "#     Args:\n",
    "#         optimizer: The optimizer object used in inner loop for fast weight updates.\n",
    "#         alpha:     The learning rate for slow weight update.\n",
    "#                    Default: 0.5\n",
    "#         k:         Number of iterations of fast weights updates before updating slow\n",
    "#                    weights.\n",
    "#                    Default: 5\n",
    "\n",
    "#     Example:\n",
    "#         > optim = Lookahead(optimizer)\n",
    "#         > optim = Lookahead(optimizer, alpha=0.6, k=10)\n",
    "#     '''\n",
    "#     def __init__(self, optimizer, alpha=0.5, k=5):\n",
    "#         assert(0.0 <= alpha <= 1.0)\n",
    "#         assert(k >= 1)\n",
    "#         self.optimizer = optimizer\n",
    "#         self.alpha = alpha\n",
    "#         self.k = k\n",
    "#         self.k_counter = 0\n",
    "#         self.param_groups = self.optimizer.param_groups\n",
    "#         self.state = defaultdict(dict)\n",
    "#         self.slow_weights = [[param.clone().detach() for param in group['params']] for group in self.param_groups]\n",
    "    \n",
    "#     def step(self, closure=None):\n",
    "#         loss = self.optimizer.step(closure)\n",
    "#         self.k_counter += 1\n",
    "#         if self.k_counter >= self.k:\n",
    "#             for group, slow_weight in zip(self.param_groups, self.slow_weights):\n",
    "#                 for param, weight in zip(group['params'], slow_weight):\n",
    "#                     weight.data.add_(self.alpha, (param.data - weight.data))\n",
    "#                     param.data.copy_(weight.data)\n",
    "#             self.k_counter = 0\n",
    "#         return loss\n",
    "\n",
    "#     def __getstate__(self):\n",
    "#         return {\n",
    "#             'state': self.state,\n",
    "#             'optimizer': self.optimizer,\n",
    "#             'alpha': self.alpha,\n",
    "#             'k': self.k,\n",
    "#             'k_counter': self.k_counter\n",
    "#         }\n",
    "\n",
    "#     def state_dict(self):\n",
    "#         return self.optimizer.state_dict()\n",
    "\n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         self.optimizer.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31dc68a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:55.972977Z",
     "iopub.status.busy": "2023-03-24T10:59:55.972465Z",
     "iopub.status.idle": "2023-03-24T10:59:55.986909Z",
     "shell.execute_reply": "2023-03-24T10:59:55.985394Z"
    },
    "papermill": {
     "duration": 0.030842,
     "end_time": "2023-03-24T10:59:55.989835",
     "exception": false,
     "start_time": "2023-03-24T10:59:55.958993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_cross_entropy(probability, truth):\n",
    "    p = np.clip(probability,1e-4,1-1e-4)\n",
    "    logp = -np.log(p)\n",
    "    loss = logp[np.arange(len(logp)),truth]\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def do_valid(net, valid_loader, iteration):\n",
    "\n",
    "    valid_num = 0\n",
    "    valid_sign = []\n",
    "    valid_loss = 0\n",
    "\n",
    "    net = net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "    \n",
    "        net.output_type = ['inference']\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled = True):\n",
    "\n",
    "                batch_size = len(batch['index'])\n",
    "                batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n",
    "                output = net(batch) #data_parallel(net, batch) #\n",
    "\n",
    "        valid_sign.append(output['sign'].cpu().numpy())\n",
    "        valid_num += batch_size\n",
    "\n",
    "        #---\n",
    "        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n",
    "        #if valid_num==200*4: break\n",
    "\n",
    "    #print('')\n",
    "    assert(valid_num == len(valid_loader.dataset))\n",
    "    #------\n",
    "    truth = valid_loader.dataset.df.label.values\n",
    "    sign = np.concatenate(valid_sign)\n",
    "    predict = np.argsort(-sign, -1)\n",
    "    correct = predict==truth.reshape(valid_num,1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "    loss = np_cross_entropy(sign, truth)\n",
    "\n",
    "    return [loss, topk[0], topk[1],  topk[4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bca8c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:56.016657Z",
     "iopub.status.busy": "2023-03-24T10:59:56.015679Z",
     "iopub.status.idle": "2023-03-24T10:59:56.045607Z",
     "shell.execute_reply": "2023-03-24T10:59:56.044234Z"
    },
    "papermill": {
     "duration": 0.046697,
     "end_time": "2023-03-24T10:59:56.048670",
     "exception": false,
     "start_time": "2023-03-24T10:59:56.001973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "from torch.optim import RAdam, AdamW\n",
    "\n",
    "fold_type = 'kaggle-part'\n",
    "fold_type = 'christ-part'\n",
    "\n",
    "fold     = 2\n",
    "out_dir  = '/kaggle/input' + '/result/run10/transfomer-80-256-lip-hand-3a'\n",
    "fold_dir = out_dir+ f'/fold-{fold}-{fold_type}'\n",
    "\n",
    "#https://www.kaggle.com/competitions/asl-signs/discussion/391203\n",
    "def run_train():\n",
    "    start_lr   = 1e-4 #0.0001\n",
    "    batch_size = 64    #6 #32\n",
    "    skip_save_epoch = 0\n",
    "    num_epoch = 200\n",
    "    \n",
    "\n",
    "# log = Logger()\n",
    "# log.open(fold_dir+'/log.train.txt',mode='a')\n",
    "# log.write(f'\\n--- [START {log.timestamp()}] {\"-\"*64}\\n\\n')\n",
    "# log.write(f'\\t{set_environment()}\\n')\n",
    "# log.write(f'\\t__file__ = {__file__}\\n')\n",
    "# log.write(f'\\tfold_dir = {fold_dir}\\n')\n",
    "# log.write(f'\\n')\n",
    "\n",
    "\n",
    "## dataset ----------------------------------------\n",
    "# log.write('** dataset setting **\\n')\n",
    "\n",
    "    if fold_type == 'kaggle-part':\n",
    "        train_df, valid_df = read_kaggle_csv_by_part(fold)\n",
    "    if fold_type == 'christ-part':\n",
    "        train_df, valid_df = read_christ_csv_by_part(fold)\n",
    "\n",
    "    #train_df, valid_df = read_kaggle_random_csv(fold)\n",
    "    train_dataset = SignDataset(train_df,train_augment)\n",
    "    valid_dataset = SignDataset(valid_df,)\n",
    "    \n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        #sampler = BalanceSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 16,\n",
    "        pin_memory  = False,\n",
    "        worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "        collate_fn = null_collate,\n",
    "    )\n",
    " \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler = SequentialSampler(valid_dataset),\n",
    "        batch_size  = 64,\n",
    "        drop_last   = False,\n",
    "        num_workers = 16,\n",
    "        pin_memory  = False,\n",
    "        collate_fn = null_collate,\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    net = Net()\n",
    "    #net.load_pretrain()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    start_iteration = 0\n",
    "    start_epoch = 0\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "    ## optimiser ----------------------------------\n",
    "    if 0: ##freeze\n",
    "        for p in net.encoder.parameters():   p.requires_grad = False\n",
    "        #for p in net.decoder.parameters():   p.requires_grad = False\n",
    "        pass\n",
    "\n",
    "    def freeze_bn(net):\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                m.weight.requires_grad = False\n",
    "                m.bias.requires_grad = False\n",
    "    #freeze_bn(net)\n",
    "\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n",
    "# optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr), alpha=0.5, k=5)\n",
    "\n",
    "    num_iteration = num_epoch*len(train_loader)\n",
    "    iter_log   = len(train_loader) *3\n",
    "    iter_valid = iter_log\n",
    "    iter_save  = iter_log\n",
    " \n",
    "    ## start training here! ##############################################\n",
    "    \n",
    "    def message(mode='print'):\n",
    "        asterisk = ' '\n",
    "        if mode==('print'):\n",
    "            loss = batch_loss\n",
    "        if mode==('log'):\n",
    "            loss = train_loss\n",
    "            if (iteration % iter_save == 0): asterisk = '*'\n",
    "        \n",
    "        text = \\\n",
    "            ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "            '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "            '%4.3f  %4.3f  %4.3f  | '%(*loss,) + \\\n",
    "            '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    #----\n",
    "    valid_loss = np.zeros(4,np.float32)\n",
    "    train_loss = np.zeros(3,np.float32)\n",
    "    batch_loss = np.zeros_like(train_loss)\n",
    "    sum_train_loss = np.zeros_like(train_loss)\n",
    "    sum_train = 0\n",
    "    \n",
    "\n",
    "    start_timer = timer()\n",
    "    iteration = start_iteration\n",
    "    epoch = start_epoch\n",
    "    rate = 0\n",
    "    while iteration < num_iteration:\n",
    "        for t, batch in enumerate(train_loader):\n",
    "\n",
    "            if iteration%iter_save==0:\n",
    "                if iteration != start_iteration:\n",
    "                    n = iteration if epoch > skip_save_epoch else 0\n",
    "                    torch.save({\n",
    "                        'state_dict': net.state_dict(),\n",
    "                        'iteration': iteration,\n",
    "                        'epoch': epoch,\n",
    "                    }, f'{n:08d}.model.pth')\n",
    "                    pass\n",
    "\n",
    "                    \n",
    "            if (iteration%iter_valid==0): # or (t==len(train_loader)-1):\n",
    "                if iteration!=start_iteration:\n",
    "                    valid_loss = do_valid(net, valid_loader, f'{iteration:08d}')  #\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            # learning rate schduler ------------\n",
    "            # adjust_learning_rate(optimizer, scheduler(epoch))\n",
    "            rate = get_learning_rate(optimizer) #scheduler.get_last_lr()[0] #get_learning_rate(optimizer)\n",
    "            \n",
    "            # one iteration update  -------------\n",
    "            batch_size = len(batch['index'])\n",
    "            batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n",
    "\n",
    "            net.train()\n",
    "            net.output_type = ['loss', 'inference']\n",
    "    #        with torch.autograd.set_detect_anomaly(True):\n",
    "            if 1:\n",
    "                with torch.cuda.amp.autocast(enabled = True):\n",
    "                    output = data_parallel(net,batch)#net(batch) \n",
    "                    loss0  = output['label_loss'].mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(\n",
    "                        loss0\n",
    "                ).backward()\n",
    "                \n",
    "                #scaler.unscale_(optimizer)\n",
    "                #torch.nn.utils.clip_grad_norm_(net.parameters(), 2)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            \n",
    "            # print statistics  --------\n",
    "            batch_loss[:3] = [loss0.item(),0,0]\n",
    "            sum_train_loss += batch_loss\n",
    "            sum_train += 1\n",
    "            if t % 100 == 0:\n",
    "                train_loss = sum_train_loss / (sum_train + 1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum_train = 0\n",
    "            \n",
    "            print('\\r', end='', flush=True)\n",
    "            print(message(mode='print'), end='', flush=True)\n",
    "            epoch += 1 / len(train_loader)\n",
    "            iteration += 1\n",
    "            \n",
    "            # debug  --------\n",
    "            #if 1:\n",
    "            # if t % 100 == 0:\n",
    "            # \tshow_result(batch, output, resize=0.50)\n",
    "            # \tcv2.waitKey(1)\n",
    "            \n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# main #################################################################\n",
    "# if __name__ == '__main__':\n",
    "#     run_train()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c4fb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:56.074878Z",
     "iopub.status.busy": "2023-03-24T10:59:56.073644Z",
     "iopub.status.idle": "2023-03-24T10:59:56.079429Z",
     "shell.execute_reply": "2023-03-24T10:59:56.078169Z"
    },
    "papermill": {
     "duration": 0.021638,
     "end_time": "2023-03-24T10:59:56.082029",
     "exception": false,
     "start_time": "2023-03-24T10:59:56.060391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "# net.load_state_dict(torch.load('/kaggle/input/gislr-saved-models/00025200.model.pth', map_location= torch.device('cpu'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842c05e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:56.107676Z",
     "iopub.status.busy": "2023-03-24T10:59:56.107202Z",
     "iopub.status.idle": "2023-03-24T10:59:56.600628Z",
     "shell.execute_reply": "2023-03-24T10:59:56.599140Z"
    },
    "papermill": {
     "duration": 0.510226,
     "end_time": "2023-03-24T10:59:56.603909",
     "exception": false,
     "start_time": "2023-03-24T10:59:56.093683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dum_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n",
    "dum_df = pd.DataFrame(dum_df.loc[0]).T\n",
    "dum_val_dataset = SignDataset(dum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf8f529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:56.629985Z",
     "iopub.status.busy": "2023-03-24T10:59:56.628757Z",
     "iopub.status.idle": "2023-03-24T10:59:56.638233Z",
     "shell.execute_reply": "2023-03-24T10:59:56.636909Z"
    },
    "papermill": {
     "duration": 0.025082,
     "end_time": "2023-03-24T10:59:56.640616",
     "exception": false,
     "start_time": "2023-03-24T10:59:56.615534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SignDataset at 0x7fb183161690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dum_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4df4454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:56.666398Z",
     "iopub.status.busy": "2023-03-24T10:59:56.665855Z",
     "iopub.status.idle": "2023-03-24T10:59:57.077647Z",
     "shell.execute_reply": "2023-03-24T10:59:57.075718Z"
    },
    "papermill": {
     "duration": 0.429321,
     "end_time": "2023-03-24T10:59:57.081655",
     "exception": false,
     "start_time": "2023-03-24T10:59:56.652334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "val_loader  = DataLoader(\n",
    "    dum_val_dataset,\n",
    "    sampler = RandomSampler(dum_val_dataset),\n",
    "    #sampler = BalanceSampler(train_dataset),\n",
    "    batch_size  = 64,\n",
    "    drop_last   = False,\n",
    "    num_workers = 16,\n",
    "    pin_memory  = False,\n",
    "    collate_fn = null_collate,\n",
    ")\n",
    "\n",
    "for t, batch in enumerate(val_loader):\n",
    "    batch['xyz'] = [xyz.cpu() for xyz in batch['xyz']]\n",
    "#     output = net(batch)\n",
    "#     print(torch.argmax(output['sign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09e7c4a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.109243Z",
     "iopub.status.busy": "2023-03-24T10:59:57.107687Z",
     "iopub.status.idle": "2023-03-24T10:59:57.117806Z",
     "shell.execute_reply": "2023-03-24T10:59:57.116336Z"
    },
    "papermill": {
     "duration": 0.026881,
     "end_time": "2023-03-24T10:59:57.120424",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.093543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 82, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['xyz'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac211993",
   "metadata": {
    "papermill": {
     "duration": 0.011506,
     "end_time": "2023-03-24T10:59:57.143969",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.132463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Note batch['xyz'] is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bfac2",
   "metadata": {
    "papermill": {
     "duration": 0.011343,
     "end_time": "2023-03-24T10:59:57.167050",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.155707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# converting the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd953922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.193759Z",
     "iopub.status.busy": "2023-03-24T10:59:57.192307Z",
     "iopub.status.idle": "2023-03-24T10:59:57.198619Z",
     "shell.execute_reply": "2023-03-24T10:59:57.197657Z"
    },
    "papermill": {
     "duration": 0.022275,
     "end_time": "2023-03-24T10:59:57.201207",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.178932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length= 80\n",
    "embed_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37d233",
   "metadata": {
    "papermill": {
     "duration": 0.011276,
     "end_time": "2023-03-24T10:59:57.224358",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.213082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Updated SingleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e0881b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.250891Z",
     "iopub.status.busy": "2023-03-24T10:59:57.249575Z",
     "iopub.status.idle": "2023-03-24T10:59:57.282828Z",
     "shell.execute_reply": "2023-03-24T10:59:57.281200Z"
    },
    "papermill": {
     "duration": 0.050228,
     "end_time": "2023-03-24T10:59:57.286354",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.236126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pytorch model for tflite conversion\n",
    "\n",
    "#simplfiy for one video input \n",
    "max_length = 80  #reduce this if gets out of memory error\n",
    "num_class  = 250\n",
    "num_landmark = 543\n",
    "num_point  = 82 \n",
    "class InputNet(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length \n",
    "  \n",
    "    def forward(self, xyz):\n",
    "        xyz = xyz - xyz[~torch.isnan(xyz)].mean(0,keepdim=True) #noramlisation to common maen\n",
    "        xyz = xyz / xyz[~torch.isnan(xyz)].std(0, keepdim=True)\n",
    "\n",
    "        LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "        #LHAND = np.arange(468, 489).tolist()\n",
    "        #RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "        lip = xyz[:, LIP]\n",
    "        lhand = xyz[:, 468:489]\n",
    "        rhand = xyz[:, 522:543]\n",
    "        xyz = torch.cat([  # (none, 82, 3)\n",
    "            lip,\n",
    "            lhand,\n",
    "            rhand,\n",
    "        ], 1)\n",
    "        xyz[torch.isnan(xyz)] = 0\n",
    "        x = xyz[:self.max_length]\n",
    "        return x\n",
    "\n",
    "\n",
    "#overwrite the model used in training ....\n",
    "\n",
    "# use fix dimension\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            num_head,\n",
    "            batch_first,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads=num_head,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            kdim=None,\n",
    "            vdim=None,\n",
    "            dropout=0.0,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.linear(x[:1], self.mha.in_proj_weight[:512], self.mha.in_proj_bias[:512]) #since we need only cls\n",
    "        k = F.linear(x, self.mha.in_proj_weight[512:1024], self.mha.in_proj_bias[512:1024])\n",
    "        v = F.linear(x, self.mha.in_proj_weight[1024:], self.mha.in_proj_bias[1024:]) \n",
    "        q = q.reshape(-1, 8, 64).permute(1, 0, 2)\n",
    "        k = k.reshape(-1, 8, 64).permute(1, 2, 0)\n",
    "        v = v.reshape(-1, 8, 64).permute(1, 0, 2)\n",
    "        dot  = torch.matmul(q, k) * (1/128**0.5) # H L L\n",
    "        attn = F.softmax(dot, -1)  #   L L\n",
    "        out  = torch.matmul(attn, v)  #   L H dim\n",
    "        out  = out.permute(1, 0, 2).reshape(-1, 512)\n",
    "        out  = F.linear(out, self.mha.out_proj.weight, self.mha.out_proj.bias)  \n",
    "        return out  \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "# remove mask\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        embed_dim,\n",
    "        num_head,\n",
    "        out_dim,\n",
    "        batch_first=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn  = MultiHeadAttention(embed_dim, num_head,batch_first)\n",
    "        self.ffn   = FeedForward(embed_dim, out_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "            \n",
    "        x = x[:1] + self.attn((self.norm1(x)))\n",
    "        x = x + self.ffn((self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "def positional_encoding(length, embed_dim):\n",
    "    dim = embed_dim//2\n",
    "\n",
    "    position = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    dim = np.arange(dim)[np.newaxis, :]/dim   # (1, dim)\n",
    "\n",
    "    angle = 1 / (10000**dim)         # (1, dim)\n",
    "    angle = position * angle    # (pos, dim)\n",
    "\n",
    "    pos_embed = np.concatenate(\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "        axis=-1\n",
    "    )\n",
    "    pos_embed = torch.from_numpy(pos_embed).float()\n",
    "    return pos_embed\n",
    "            \n",
    "class SingleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=num_class):\n",
    "        super().__init__()\n",
    "        self.num_block = 1\n",
    "        self.embed_dim = 512\n",
    "        self.num_head  = 8\n",
    "        self.max_length = max_length\n",
    "        self.num_point = num_point\n",
    "\n",
    "        pos_embed = positional_encoding(max_length, self.embed_dim)\n",
    "        self.pos_embed = nn.Parameter(pos_embed)\n",
    "\n",
    "        self.cls_embed = nn.Parameter(torch.zeros((1, self.embed_dim)))\n",
    "        self.x_embed = nn.Sequential(\n",
    "            nn.Linear(num_point * 3, self.embed_dim, bias=False),\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                self.embed_dim,\n",
    "                self.num_head,\n",
    "                self.embed_dim,\n",
    "                batch_first=False\n",
    "            ) for i in range(self.num_block)\n",
    "        ])\n",
    "        self.logit = nn.Linear(self.embed_dim, num_class)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        L = xyz.shape[0]\n",
    "        x_embed = self.x_embed(xyz.flatten(1)) \n",
    "        x = x_embed[:L] + self.pos_embed[:L]\n",
    "        x = torch.cat([\n",
    "            self.cls_embed,\n",
    "            x\n",
    "        ],0)\n",
    "        #x = x.unsqueeze(1)\n",
    "\n",
    "        #for block in self.encoder: x = block(x) #remove tflite loop\n",
    "        x = self.encoder[0](x)\n",
    "        cls = x[[0]]\n",
    "        logit = self.logit(cls)\n",
    "        return logit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2764f8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.313927Z",
     "iopub.status.busy": "2023-03-24T10:59:57.312577Z",
     "iopub.status.idle": "2023-03-24T10:59:57.515915Z",
     "shell.execute_reply": "2023-03-24T10:59:57.513479Z"
    },
    "papermill": {
     "duration": 0.220582,
     "end_time": "2023-03-24T10:59:57.518859",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.298277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputnet = InputNet()\n",
    "# inputnet.load_state_dict(torch.load('/kaggle/input/gislr-saved-models/00007200.model_512_80.pth', map_location= torch.device('cpu'))['state_dict'])\n",
    "singlenet = SingleNet()\n",
    "singlenet.load_state_dict(torch.load('/kaggle/input/gislr-saved-models/00007200.model_512_80.pth', map_location= torch.device('cpu'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1abe9acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.545445Z",
     "iopub.status.busy": "2023-03-24T10:59:57.544957Z",
     "iopub.status.idle": "2023-03-24T10:59:57.566019Z",
     "shell.execute_reply": "2023-03-24T10:59:57.563683Z"
    },
    "papermill": {
     "duration": 0.038153,
     "end_time": "2023-03-24T10:59:57.569104",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.530951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87)\n"
     ]
    }
   ],
   "source": [
    "dummy_tensor = torch.zeros((23, 82, 3))\n",
    "print(torch.argmax(singlenet(dummy_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c20b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.597094Z",
     "iopub.status.busy": "2023-03-24T10:59:57.595699Z",
     "iopub.status.idle": "2023-03-24T10:59:57.607072Z",
     "shell.execute_reply": "2023-03-24T10:59:57.605746Z"
    },
    "papermill": {
     "duration": 0.027931,
     "end_time": "2023-03-24T10:59:57.610016",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.582085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25)\n"
     ]
    }
   ],
   "source": [
    "dummy_tensor2 = batch['xyz'][0]\n",
    "print(torch.argmax(singlenet(dummy_tensor2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e222adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.638114Z",
     "iopub.status.busy": "2023-03-24T10:59:57.637610Z",
     "iopub.status.idle": "2023-03-24T10:59:57.656790Z",
     "shell.execute_reply": "2023-03-24T10:59:57.655322Z"
    },
    "papermill": {
     "duration": 0.035742,
     "end_time": "2023-03-24T10:59:57.659589",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.623847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 82, 3])\n"
     ]
    }
   ],
   "source": [
    "dum = torch.zeros((100,543,3))\n",
    "print(inputnet(dum).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d6b2f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.687994Z",
     "iopub.status.busy": "2023-03-24T10:59:57.687474Z",
     "iopub.status.idle": "2023-03-24T10:59:57.697315Z",
     "shell.execute_reply": "2023-03-24T10:59:57.695676Z"
    },
    "papermill": {
     "duration": 0.026357,
     "end_time": "2023-03-24T10:59:57.699970",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.673613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # converting to onnx\n",
    "    dummy_inputnet = torch.zeros((100, 543, 3))\n",
    "    dummy_singlenet = torch.zeros((80, 82, 3))\n",
    "\n",
    "    # input_tensor = {'xyz': dummy_tensor,\n",
    "    #     'label': 'blow'} # = batch\n",
    "    singlenet.eval()\n",
    "\n",
    "    torch.onnx.export(\n",
    "        inputnet,\n",
    "        dummy_inputnet,\n",
    "        \"GISLR_model.input.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=12,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['inputs'],\n",
    "        output_names=['outputs'],\n",
    "        dynamic_axes={\n",
    "            'inputs' : {0 : 'length'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        singlenet,               \n",
    "        dummy_singlenet,                   \n",
    "        \"GISLR_model.single.onnx\",   \n",
    "        export_params=True,  \n",
    "        opset_version=12,    \n",
    "        do_constant_folding=True, \n",
    "        input_names = ['inputs'],  \n",
    "        output_names = ['outputs'],\n",
    "        dynamic_axes={\n",
    "            'inputs' : {0 : 'length'}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff56a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.726916Z",
     "iopub.status.busy": "2023-03-24T10:59:57.726408Z",
     "iopub.status.idle": "2023-03-24T10:59:57.732710Z",
     "shell.execute_reply": "2023-03-24T10:59:57.731451Z"
    },
    "papermill": {
     "duration": 0.022954,
     "end_time": "2023-03-24T10:59:57.735217",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.712263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    for f in [\"GISLR_model.input.onnx\", \"GISLR_model.single.onnx\"]:\n",
    "        if f is None: continue\n",
    "        model = onnx.load(f)\n",
    "        onnx.checker.check_model(model)\n",
    "        model_simple, check = onnxsim.simplify(model)\n",
    "        onnx.save(model_simple, f)\n",
    "    print('onnx simplify() passed !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67764171",
   "metadata": {
    "papermill": {
     "duration": 0.011551,
     "end_time": "2023-03-24T10:59:57.759007",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.747456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Checks and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c3d4553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.785626Z",
     "iopub.status.busy": "2023-03-24T10:59:57.784747Z",
     "iopub.status.idle": "2023-03-24T10:59:57.791219Z",
     "shell.execute_reply": "2023-03-24T10:59:57.789847Z"
    },
    "papermill": {
     "duration": 0.022646,
     "end_time": "2023-03-24T10:59:57.793711",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.771065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_file = '/kaggle/input/gislr-saved-models/transformer_80.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3199b0c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.820989Z",
     "iopub.status.busy": "2023-03-24T10:59:57.819736Z",
     "iopub.status.idle": "2023-03-24T10:59:57.830856Z",
     "shell.execute_reply": "2023-03-24T10:59:57.829549Z"
    },
    "papermill": {
     "duration": 0.027301,
     "end_time": "2023-03-24T10:59:57.833364",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.806063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ok\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "mode = 'submit'\n",
    "\n",
    "if mode in ['debug']:  \n",
    "    try:\n",
    "        import tflite_runtime\n",
    "    except:\n",
    "        !pip install tflite-runtime\n",
    "\n",
    "    import tflite_runtime.interpreter as tflite   \n",
    "    import tflite_runtime\n",
    "    print(tflite_runtime.__version__)\n",
    "    #'2.11.0'\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    print(tf.__version__)\n",
    "    # 2.11.0\n",
    "\n",
    "print('import ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6209b919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:59:57.860683Z",
     "iopub.status.busy": "2023-03-24T10:59:57.859697Z",
     "iopub.status.idle": "2023-03-24T11:00:00.700852Z",
     "shell.execute_reply": "2023-03-24T11:00:00.698829Z"
    },
    "papermill": {
     "duration": 2.85877,
     "end_time": "2023-03-24T11:00:00.704167",
     "exception": false,
     "start_time": "2023-03-24T10:59:57.845397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.tflite (deflated 8%)\r\n",
      "__notebook__.ipynb  model.tflite  submission.zip\r\n",
      "tflite_file: /kaggle/input/gislr-saved-models/transformer_80.tflite\n",
      "submit ok\n"
     ]
    }
   ],
   "source": [
    "#helper functions\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "if mode in ['debug']: \n",
    " \n",
    "    interpreter = tflite.Interpreter(tflite_file)\n",
    "    prediction_fn = interpreter.get_signature_runner('serving_default')\n",
    "\n",
    "    valid_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv') \n",
    "    valid_df = valid_df[valid_df.fold==2].reset_index(drop=True)\n",
    "    valid_df = valid_df[:4_000]\n",
    "    valid_num = len(valid_df)\n",
    "    valid = {\n",
    "        'sign':[],\n",
    "    }\n",
    "    \n",
    "    start_timer = timer()\n",
    "    for t, d in valid_df.iterrows():\n",
    "\n",
    "        pq_file = f'/kaggle/input/asl-signs/{d.path}'\n",
    "        #print(pq_file)\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "\n",
    "        output = prediction_fn(inputs=xyz)\n",
    "        p = output['outputs'].reshape(-1)\n",
    "\n",
    "        valid['sign'].append(p)\n",
    "\n",
    "        #---\n",
    "        if t%100==0:\n",
    "            time_taken = timer() - start_timer\n",
    "            print('\\r %8d / %d  %s'%(t,valid_num,time_to_str(time_taken,'sec')),end='',flush=True)\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    truth = valid_df.label.values\n",
    "    sign  = np.stack(valid['sign'])\n",
    "    predict = np.argsort(-sign, -1)\n",
    "    correct = predict==truth.reshape(valid_num,1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "\n",
    "    print(f'time_taken = {time_to_str(time_taken,\"sec\")}')\n",
    "    print(f'time_taken for LB = {time_taken*1000/valid_num:05f} msec\\n')\n",
    "    for i in range(5):\n",
    "        print(f'topk[{i}] = {topk[i]}')  \n",
    "    print('----- end -----\\n')\n",
    "\n",
    "shutil.copyfile(tflite_file, 'model.tflite') \n",
    "!zip submission.zip  'model.tflite'\n",
    "!ls\n",
    "\n",
    "print('tflite_file:', tflite_file)\n",
    "print(f'submit ok')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8720362",
   "metadata": {
    "papermill": {
     "duration": 0.011892,
     "end_time": "2023-03-24T11:00:00.728578",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.716686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Skip the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1abb43a9",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-24T11:00:00.756753Z",
     "iopub.status.busy": "2023-03-24T11:00:00.756219Z",
     "iopub.status.idle": "2023-03-24T11:00:00.762522Z",
     "shell.execute_reply": "2023-03-24T11:00:00.761082Z"
    },
    "papermill": {
     "duration": 0.023884,
     "end_time": "2023-03-24T11:00:00.765456",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.741572",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install onnxsim\n",
    "# !pip install onnxruntime\n",
    "# !pip install onnx_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80006244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T11:00:00.792761Z",
     "iopub.status.busy": "2023-03-24T11:00:00.792267Z",
     "iopub.status.idle": "2023-03-24T11:00:00.797500Z",
     "shell.execute_reply": "2023-03-24T11:00:00.796091Z"
    },
    "papermill": {
     "duration": 0.022278,
     "end_time": "2023-03-24T11:00:00.800311",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.778033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# import onnxruntime\n",
    "# import onnxsim\n",
    "# from onnx_tf.backend import prepare\n",
    "\n",
    "# # model = onnx.load('/kaggle/working/GISLR_model.onnx') # if not already saved, uncomment this\n",
    "# model = onnx.load('/kaggle/input/gislr-saved-models/GISLR_model.onnx')\n",
    "# onnx.checker.check_model(model)\n",
    "# model_simple, check = onnxsim.simplify(model)\n",
    "# onnx.save(model_simple, \"GISLR_model_simple.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "383b3bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T11:00:00.829601Z",
     "iopub.status.busy": "2023-03-24T11:00:00.828464Z",
     "iopub.status.idle": "2023-03-24T11:00:00.834347Z",
     "shell.execute_reply": "2023-03-24T11:00:00.833138Z"
    },
    "papermill": {
     "duration": 0.023708,
     "end_time": "2023-03-24T11:00:00.836949",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.813241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # converting onnx simplified to tflite\n",
    "# tf_rep = prepare(onnx.load('/kaggle/input/gislr-saved-models/GISLR_model_simple.onnx'))\n",
    "# tf_rep.export_graph('')\n",
    "# # done these files created: variables/, assets/, saved_model.pb, fingerprint.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c7de23e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T11:00:00.864670Z",
     "iopub.status.busy": "2023-03-24T11:00:00.864208Z",
     "iopub.status.idle": "2023-03-24T11:00:00.870323Z",
     "shell.execute_reply": "2023-03-24T11:00:00.869237Z"
    },
    "papermill": {
     "duration": 0.02309,
     "end_time": "2023-03-24T11:00:00.872765",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.849675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# class TFModel(tf.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.max_length = 80\n",
    "#         self.model = tf.saved_model.load('')\n",
    "#         self.model.trainable = False\n",
    "    \n",
    "#     @tf.function(input_signature=[\n",
    "#         tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')\n",
    "#     ])\n",
    "    \n",
    "#     def call(self, xyz):\n",
    "# #         xyz_mean = tf.math.reduce_mean(xyz[~tf.math.is_nan(xyz)], axis=(0, 1, 2), keepdims=True)\n",
    "# #         xyz_std = tf.math.reduce_std(xyz[~tf.math.is_nan(xyz)], axis=(0, 1, 2), keepdims=True)\n",
    "# #         xyz = (xyz - xyz_mean) / xyz_std  # normalize to common mean and std\n",
    "#         nan_mask = tf.math.is_nan(xyz)\n",
    "#         valid_mask = ~nan_mask\n",
    "#         masked_xyz = tf.boolean_mask(xyz, valid_mask)\n",
    "#         xyz_mean = tf.math.reduce_mean(masked_xyz, axis=0, keepdims=True)\n",
    "#         xyz_std = tf.math.reduce_std(masked_xyz, axis=0, keepdims=True)\n",
    "#         normalized_xyz = (xyz - xyz_mean) / xyz_std\n",
    "#         xyz = tf.where(nan_mask, tf.zeros_like(xyz), normalized_xyz)\n",
    "\n",
    "#         LIP = [\n",
    "#             61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "#             291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "#             78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "#             95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "#         ]\n",
    "        \n",
    "#         lip_indices = tf.constant(LIP, dtype=tf.int32)\n",
    "#         #LHAND = np.arange(468, 489).tolist()\n",
    "#         #RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "# #         lip = xyz[:, LIP]\n",
    "#         lip = tf.gather(xyz, lip_indices, axis=1)\n",
    "#         lhand = xyz[:, 468:489]\n",
    "#         rhand = xyz[:, 522:543]\n",
    "#         xyz = tf.concat([  # (none, 82, 3)\n",
    "#             lip,\n",
    "#             lhand,\n",
    "#             rhand,\n",
    "#         ], 1)\n",
    "#         xyz = tf.where(tf.math.is_nan(xyz), tf.zeros_like(xyz), xyz)  # set NaN values to zero\n",
    "#         x = xyz[:self.max_length]\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         y={}\n",
    "#         y['outputs'] = self.model(**{'inputs':x})['outputs'][0]\n",
    "#         return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d37e846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T11:00:00.901953Z",
     "iopub.status.busy": "2023-03-24T11:00:00.901442Z",
     "iopub.status.idle": "2023-03-24T11:00:00.906762Z",
     "shell.execute_reply": "2023-03-24T11:00:00.905171Z"
    },
    "papermill": {
     "duration": 0.022566,
     "end_time": "2023-03-24T11:00:00.909381",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.886815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tfmodel = TFModel()\n",
    "# tf.saved_model.save(tfmodel, \"\", signatures={'serving_default':tfmodel.call})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae5d90",
   "metadata": {
    "papermill": {
     "duration": 0.011965,
     "end_time": "2023-03-24T11:00:00.934206",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.922241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f8994",
   "metadata": {
    "papermill": {
     "duration": 0.01188,
     "end_time": "2023-03-24T11:00:00.958576",
     "exception": false,
     "start_time": "2023-03-24T11:00:00.946696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.982557,
   "end_time": "2023-03-24T11:00:03.474454",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-24T10:59:37.491897",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
