{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":24.713105,"end_time":"2023-06-03T09:29:10.017421","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-06-03T09:28:45.304316","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Version 3**: adding inference for saved models\n### **Version 4**: adding fine-tuning pipeline","metadata":{"papermill":{"duration":0.007505,"end_time":"2023-06-03T09:28:55.716094","exception":false,"start_time":"2023-06-03T09:28:55.708589","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n\nimport json\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn","metadata":{"papermill":{"duration":3.05035,"end_time":"2023-06-03T09:28:58.773126","exception":false,"start_time":"2023-06-03T09:28:55.722776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:29.054124Z","iopub.execute_input":"2023-06-04T19:59:29.054856Z","iopub.status.idle":"2023-06-04T19:59:29.063914Z","shell.execute_reply.started":"2023-06-04T19:59:29.054828Z","shell.execute_reply":"2023-06-04T19:59:29.061636Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"max_length = 80\nnum_point  = 82\n\nembed_dim  = 512\nnum_head   = 4\nnum_block  = 1\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"papermill":{"duration":0.074687,"end_time":"2023-06-03T09:28:58.854752","exception":false,"start_time":"2023-06-03T09:28:58.780065","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:47.134468Z","iopub.execute_input":"2023-06-04T19:59:47.134826Z","iopub.status.idle":"2023-06-04T19:59:47.161575Z","shell.execute_reply.started":"2023-06-04T19:59:47.134799Z","shell.execute_reply":"2023-06-04T19:59:47.160634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nnum_class  = 250\nnum_landmark = 543\n\nclass HardSwish(nn.Module):\n    def __init__(self,):\n        super().__init__()\n    def forward(self, x):\n        return x * F.relu6(x+3) * 0.16666667\n\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim, hidden_dim):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, embed_dim),\n        )\n    def forward(self, x):\n        return self.mlp(x)\n\n#https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,\n            embed_dim,\n            num_head,\n            batch_first,\n        ):\n        super().__init__()\n        self.mha = nn.MultiheadAttention(\n            embed_dim,\n            num_heads=num_head,\n            bias=True,\n            add_bias_kv=False,\n            kdim=None,\n            vdim=None,\n            dropout=0.0,\n            batch_first=batch_first,\n        )\n\n    def forward(self, x, x_mask):\n        out, _ = self.mha(x,x,x, key_padding_mask=x_mask)\n        return out\n\nclass TransformerBlock(nn.Module):\n    def __init__(self,\n        embed_dim,\n        num_head,\n        out_dim,\n        batch_first=True,\n    ):\n        super().__init__()\n        self.attn  = MultiHeadAttention(embed_dim, num_head,batch_first)\n        self.ffn   = FeedForward(embed_dim, out_dim)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(out_dim)\n\n    def forward(self, x, x_mask=None):\n        x = x + self.attn((self.norm1(x)), x_mask)\n        x = x + self.ffn((self.norm2(x)))\n        return x\n\n\ndef positional_encoding(length, embed_dim):\n    dim = embed_dim//2\n    position = np.arange(length)[:, np.newaxis]     # (seq, 1)\n    dim = np.arange(dim)[np.newaxis, :]/dim   # (1, dim)\n    angle = 1 / (10000**dim)         # (1, dim)\n    angle = position * angle    # (pos, dim)\n    pos_embed = np.concatenate(\n        [np.sin(angle), np.cos(angle)],\n        axis=-1\n    )\n    pos_embed = torch.from_numpy(pos_embed).float()\n    return pos_embed\n\ndef pack_seq(\n    seq,\n):\n    length = [min(s.shape[0], max_length)  for s in seq]\n    batch_size = len(seq)\n    K = seq[0].shape[1]\n    L = max(length)\n\n    x = torch.zeros((batch_size, L, K, 3)).to(seq[0].device)\n    x_mask = torch.zeros((batch_size, L)).to(seq[0].device)\n    for b in range(batch_size):\n        l = length[b]\n        x[b, :l] = seq[b][:l]\n        x_mask[b, l:] = 1\n    x_mask = (x_mask>0.5)\n    x = x.reshape(batch_size,-1,K*3)\n    return x, x_mask\n\n#########################################################################\n\nclass Net(nn.Module):\n\n    def __init__(self, num_class=num_class):\n        super().__init__()\n        self.output_type = ['inference', 'loss']\n\n        pos_embed = positional_encoding(max_length, embed_dim)\n        # self.register_buffer('pos_embed', pos_embed)\n        self.pos_embed = nn.Parameter(pos_embed)\n\n        self.cls_embed = nn.Parameter(torch.zeros((1, embed_dim)))\n        self.x_embed = nn.Sequential(\n            nn.Linear(num_point * 3, embed_dim, bias=False),\n        )\n\n        self.encoder = nn.ModuleList([\n            TransformerBlock(\n                embed_dim,\n                num_head,\n                embed_dim,\n            ) for i in range(num_block)\n        ])\n        self.logit = nn.Linear(embed_dim, num_class)\n\n    def forward(self, batch):\n        xyz = batch['xyz']\n        x, x_mask = pack_seq(xyz)\n        #print(x.shape, x_mask.shape)\n        B,L,_ = x.shape\n        x = self.x_embed(x)\n        x = x + self.pos_embed[:L].unsqueeze(0)\n\n        x = torch.cat([\n            self.cls_embed.unsqueeze(0).repeat(B,1,1),\n            x\n        ],1)\n        x_mask = torch.cat([\n            torch.zeros(B,1).to(x_mask),\n            x_mask\n        ],1)\n\n\n        #x = F.dropout(x,p=0.25,training=self.training)\n        for block in self.encoder:\n            x = block(x,x_mask)\n\n        cls = x[:,0]\n        cls = F.dropout(cls,p=0.4,training=self.training)\n        logit = self.logit(cls)\n\n        output = {}\n        if 'loss' in self.output_type:\n            output['label_loss'] = F.cross_entropy(logit, batch['label'])\n\n        if 'inference' in self.output_type:\n            output['sign'] = torch.softmax(logit,-1)\n\n        return output\n\n\n\n\n\ndef run_check_net():\n\n    length = [3,4]\n    batch_size = len(length)\n    xyz = [\n        np.random.uniform(-1,1,(length[b],num_point,3)) for b in range(batch_size)\n    ]\n    #---\n    batch = {\n        'label' : torch.from_numpy( np.random.choice(250,(batch_size))).long(),\n        'xyz' : [torch.from_numpy(x).float() for x in xyz]\n    }\n\n    net = Net()\n    output = net(batch)\n\n\n    #---\n    \"\"\"\n\n    print('batch')\n    for k, v in batch.items():\n        if k in ['label','x']:\n            print(f'{k:>32} : {v.shape} ')\n        if k=='xyz':\n            print(f'{k:>32} : {v[0].shape} ')\n            for i in range(1,len(v)):\n                print(f'{\" \":>32} : {v[i].shape} ')\n\n    print('output')\n    for k, v in output.items():\n        if 'loss' not in k:\n            print(f'{k:>32} : {v.shape} ')\n    print(output['sign'])\n    print('loss')\n    for k, v in output.items():\n        if 'loss' in k:\n            print(f'{k:>32} : {v.item()} ')\n\n\"\"\"\n\n# main #################################################################\nif __name__ == '__main__':\n    run_check_net()","metadata":{"papermill":{"duration":0.206317,"end_time":"2023-06-03T09:28:59.068794","exception":false,"start_time":"2023-06-03T09:28:58.862477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:48.198956Z","iopub.execute_input":"2023-06-04T19:59:48.199305Z","iopub.status.idle":"2023-06-04T19:59:48.399093Z","shell.execute_reply.started":"2023-06-04T19:59:48.199277Z","shell.execute_reply":"2023-06-04T19:59:48.398141Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.006349,"end_time":"2023-06-03T09:28:59.082077","exception":false,"start_time":"2023-06-03T09:28:59.075728","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# additional helper functions\nROWS_PER_FRAME = 543\ndef load_relevant_data_subset(pq_path, type='parquet'):\n    data_columns = ['x', 'y', 'z']\n    if type=='parquet':    \n        data = pd.read_parquet(pq_path, columns=data_columns)\n    else:\n        data = pd.read_csv(pq_path, usecols=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)","metadata":{"papermill":{"duration":0.016488,"end_time":"2023-06-03T09:28:59.105131","exception":false,"start_time":"2023-06-03T09:28:59.088643","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:49.071705Z","iopub.execute_input":"2023-06-04T19:59:49.072695Z","iopub.status.idle":"2023-06-04T19:59:49.079028Z","shell.execute_reply.started":"2023-06-04T19:59:49.072630Z","shell.execute_reply":"2023-06-04T19:59:49.078139Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom torch.utils.data import SequentialSampler, RandomSampler\n\ndef read_kaggle_csv_by_random(fold=0):\n    num_fold = 5\n\n    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n    train_df = kaggle_df[kaggle_df.fold!=fold].reset_index(drop=True)\n    valid_df = kaggle_df[kaggle_df.fold==fold].reset_index(drop=True)\n    return train_df, valid_df\n\ndef read_kaggle_csv_by_part(fold=0):\n    num_fold = 5\n\n    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n    kaggle_df.loc[:, 'fold' ] = -1\n\n    sgkf = StratifiedGroupKFold(n_splits=num_fold, random_state=123, shuffle=True)\n    for i, (train_index, valid_index) in enumerate(sgkf.split(kaggle_df.path, kaggle_df.label, kaggle_df.participant_id)):\n        kaggle_df.loc[valid_index,'fold'] = i\n\n    #kaggle_df.loc[:, 'fold'] = np.arange(len(kaggle_df))%num_fold\n    train_df = kaggle_df[kaggle_df.fold!=fold].reset_index(drop=True)\n    valid_df = kaggle_df[kaggle_df.fold==fold].reset_index(drop=True)\n    return train_df, valid_df\n\ndef read_christ_csv_by_part(fold=0):\n    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n    christ_df = kaggle_df\n    \n    christ_df = christ_df.merge(kaggle_df[['path']], on='path',validate='1:1') # also kaggle_df['num_frame'] was there removed it\n    valid_df = christ_df[christ_df.fold==fold].reset_index(drop=True)\n    train_df = christ_df[christ_df.fold!=fold].reset_index(drop=True)\n    return train_df, valid_df\n\n\ndef pre_process(xyz):\n    #xyz = xyz - xyz[~torch.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n    #xyz = xyz / xyz[~torch.isnan(xyz)].std(0, keepdims=True)\n    \n    LIP = [\n            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n        ]\n    \n    lip   = xyz[:, LIP]\n    lhand = xyz[:, 468:489]\n    rhand = xyz[:, 522:543]\n    xyz = torch.cat([ #(none, 82, 3)\n        lip,\n        lhand,\n        rhand,\n    ],1)\n    xyz[torch.isnan(xyz)] = 0\n    xyz = xyz[:max_length]\n    return xyz\n\n\n#-----------------------------------------------------\ndef train_augment(xyz):\n    xyz = do_random_affine(\n        xyz,\n        scale  = (0.7,1.3),\n        shift  = (-0.08,0.08),\n        degree = (-20,20),\n        p=0.8\n    )\n    return xyz\n\n\nclass SignDataset(Dataset):\n    def __init__(self, df, augment=None):\n        self.df = df\n        self.augment = augment\n        self.length = len(self.df)\n\n    def __str__(self):\n        num_participant_id = self.df.participant_id.nunique()\n        string = ''\n        string += f'\\tlen = {len(self)}\\n'\n        string += f'\\tnum_participant_id = {num_participant_id}\\n'\n        return string\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n\n        pq_file = f'/kaggle/input/asl-signs/{d.path}'\n        xyz = load_relevant_data_subset(pq_file)\n#         print(xyz)\n        xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n        xyz = xyz / xyz[~np.isnan(xyz)].std(0, keepdims=True)\n\n        #--\n#         if self.augment is not None:\n#             xyz = self.augment(xyz)\n#         print(xyz)\n        #--\n        xyz = torch.from_numpy(xyz).float()\n        xyz = pre_process(xyz)\n\n        r = {}\n        r['index'] = index\n        r['d'    ] = d\n        r['xyz'  ] = xyz\n        r['label'] = d.label\n        return r\n\n\ntensor_key = ['xyz', 'label']\ndef null_collate(batch):\n    batch_size = len(batch)\n    d = {}\n    key = batch[0].keys()\n    for k in key:\n        d[k] = [b[k] for b in batch]\n    d['label'] = torch.LongTensor(d['label'])\n    return d\n\n\n\n\n#################################################################################\n\ndef run_check_dataset():\n\n    train_df, valid_df = read_kaggle_csv_by_part(fold=0)\n    dataset = SignDataset(valid_df)\n    print(dataset)\n\n    for i in range(12):\n        r = dataset[i]\n        print(r['index'], '--------------------')\n        print(r[\"d\"], '\\n')\n        for k in tensor_key:\n            if k =='label': continue\n            v = r[k]\n            print(k)\n            print('\\t', 'dtype:', v.dtype)\n            print('\\t', 'shape:', v.shape)\n            if len(v)!=0:\n                print('\\t', 'min/max:', v.min().item(),'/', v.max().item())\n                print('\\t', 'is_contiguous:', v.is_contiguous())\n                print('\\t', 'values:')\n                print('\\t\\t', v.reshape(-1)[:5].data.numpy().tolist(), '...')\n                print('\\t\\t', v.reshape(-1)[-5:].data.numpy().tolist())\n        print('')\n        if 0:\n            #draw\n            cv2.waitKey(1)\n\n\n\n    loader = DataLoader(\n        dataset,\n        sampler=SequentialSampler(dataset),\n        batch_size=8,\n        drop_last=True,\n        num_workers=0,\n        pin_memory=False,\n        worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n        collate_fn=null_collate,\n    )\n    print(f'batch_size   : {loader.batch_size}')\n    print(f'len(loader)  : {len(loader)}')\n    print(f'len(dataset) : {len(dataset)}')\n    print('')\n\n    for t, batch in enumerate(loader):\n        if t > 5: break\n        print('batch ', t, '===================')\n        print('index', batch['index'])\n\n        for k in tensor_key:\n            v = batch[k]\n\n            if k =='label':\n                print('label:')\n                print('\\t', v.data.numpy().tolist())\n\n            if k =='x':\n                print('x:')\n                print('\\t', v.data.shape)\n\n            if k =='xyz':\n                print('xyz:')\n                for i in range(len(v)):\n                    print('\\t', v[i].shape)\n\n        if 1:\n            pass\n        print('')\n\n\n# main #################################################################\nif __name__ == '__main__':\n    run_check_dataset()","metadata":{"_kg_hide-output":true,"papermill":{"duration":3.569001,"end_time":"2023-06-03T09:29:02.680563","exception":false,"start_time":"2023-06-03T09:28:59.111562","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:49.132818Z","iopub.execute_input":"2023-06-04T19:59:49.133150Z","iopub.status.idle":"2023-06-04T19:59:51.783025Z","shell.execute_reply.started":"2023-06-04T19:59:49.133124Z","shell.execute_reply":"2023-06-04T19:59:51.782076Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/247179138.py:17: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  kaggle_df.loc[:, 'fold' ] = -1\n","output_type":"stream"},{"name":"stdout","text":"\tlen = 22959\n\tnum_participant_id = 5\n\n0 --------------------\npath                      train_landmark_files/49445/1000397667.parquet\nparticipant_id                                                    49445\nsequence_id                                                  1000397667\nsign                                                             vacuum\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                               231\nfold                                                                  0\nName: 0, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([33, 82, 3])\n\t min/max: -1.4137358665466309 / 1.5335360765457153\n\t is_contiguous: True\n\t values:\n\t\t [0.4290144741535187, 0.43524935841560364, -0.8092206120491028, 0.4351654648780823, 0.4245232045650482] ...\n\t\t [1.5183755159378052, -0.9306944608688354, 0.30079182982444763, 1.5102177858352661, -0.8800152540206909]\n\n1 --------------------\npath                      train_landmark_files/61333/1000909322.parquet\nparticipant_id                                                    61333\nsequence_id                                                  1000909322\nsign                                                              shirt\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                               195\nfold                                                                  0\nName: 1, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([22, 82, 3])\n\t min/max: -1.6253658533096313 / 2.0139806270599365\n\t is_contiguous: True\n\t values:\n\t\t [0.1677067130804062, 0.7675708532333374, -0.8798531293869019, 0.16568496823310852, 0.7492659091949463] ...\n\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n\n2 --------------------\npath                       train_landmark_files/4718/1001385785.parquet\nparticipant_id                                                     4718\nsequence_id                                                  1001385785\nsign                                                              clown\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                49\nfold                                                                  0\nName: 2, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([80, 82, 3])\n\t min/max: -1.2869410514831543 / 1.0447239875793457\n\t is_contiguous: True\n\t values:\n\t\t [0.34457674622535706, 0.5869734883308411, -0.9500903487205505, 0.3376515805721283, 0.5622150301933289] ...\n\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n\n3 --------------------\npath                      train_landmark_files/49445/1001499433.parquet\nparticipant_id                                                    49445\nsequence_id                                                  1001499433\nsign                                                              store\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                               207\nfold                                                                  0\nName: 3, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([6, 82, 3])\n\t min/max: -0.9011333584785461 / 0.7579073309898376\n\t is_contiguous: True\n\t values:\n\t\t [0.4424348771572113, 0.3435496389865875, -0.7855185270309448, 0.45140770077705383, 0.33394286036491394] ...\n\t\t [0.3738210201263428, -0.8777258992195129, -0.25246328115463257, 0.4281977713108063, -0.8668199181556702]\n\n4 --------------------\npath                      train_landmark_files/61333/1001819372.parquet\nparticipant_id                                                    61333\nsequence_id                                                  1001819372\nsign                                                            balloon\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                14\nfold                                                                  0\nName: 4, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([27, 82, 3])\n\t min/max: -1.4120255708694458 / 1.9696542024612427\n\t is_contiguous: True\n\t values:\n\t\t [0.348316490650177, 0.5634872913360596, -1.008692979812622, 0.36670276522636414, 0.5481652021408081] ...\n\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n\n5 --------------------\npath                       train_landmark_files/2044/1001950812.parquet\nparticipant_id                                                     2044\nsequence_id                                                  1001950812\nsign                                                               milk\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                               142\nfold                                                                  0\nName: 5, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([27, 82, 3])\n\t min/max: -1.4269397258758545 / 1.64667809009552\n\t is_contiguous: True\n\t values:\n\t\t [0.35638466477394104, 0.5107186436653137, -0.9226750135421753, 0.36864763498306274, 0.5006067156791687] ...\n\t\t [1.486141324043274, -1.1071124076843262, -0.10997430980205536, 1.522465467453003, -1.0708894729614258]\n\n6 --------------------\npath                      train_landmark_files/49445/1002020383.parquet\nparticipant_id                                                    49445\nsequence_id                                                  1002020383\nsign                                                             drawer\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                62\nfold                                                                  0\nName: 6, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([60, 82, 3])\n\t min/max: -1.342134714126587 / 1.406419277191162\n\t is_contiguous: True\n\t values:\n\t\t [0.6350717544555664, 0.24549315869808197, -0.7391493320465088, 0.6425521969795227, 0.23781165480613708] ...\n\t\t [1.3408353328704834, -0.7479751706123352, 0.5829577445983887, 1.3471013307571411, -0.7190732359886169]\n\n7 --------------------\npath                      train_landmark_files/61333/1002052130.parquet\nparticipant_id                                                    61333\nsequence_id                                                  1002052130\nsign                                                                 TV\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                 0\nfold                                                                  0\nName: 7, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([80, 82, 3])\n\t min/max: -1.5962445735931396 / 2.010420322418213\n\t is_contiguous: True\n\t values:\n\t\t [0.16069254279136658, 0.6612129807472229, -1.006607174873352, 0.17581970989704132, 0.6479062438011169] ...\n\t\t [0.0, 0.0, 0.0, 0.0, 0.0]\n\n8 --------------------\npath                       train_landmark_files/2044/1002091184.parquet\nparticipant_id                                                     2044\nsequence_id                                                  1002091184\nsign                                                               duck\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                67\nfold                                                                  0\nName: 8, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([18, 82, 3])\n\t min/max: -1.3234901428222656 / 1.0862078666687012\n\t is_contiguous: True\n\t values:\n\t\t [0.3130979835987091, 0.5779224634170532, -0.8698641061782837, 0.3187340795993805, 0.563902735710144] ...\n\t\t [0.9609947800636292, -1.1268714666366577, -0.0030901161953806877, 0.9988311529159546, -1.0945731401443481]\n\n9 --------------------\npath                       train_landmark_files/2044/1002092995.parquet\nparticipant_id                                                     2044\nsequence_id                                                  1002092995\nsign                                                               blow\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                25\nfold                                                                  0\nName: 9, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([6, 82, 3])\n\t min/max: -1.0173239707946777 / 1.2186691761016846\n\t is_contiguous: True\n\t values:\n\t\t [0.24501162767410278, 0.6447877883911133, -0.8113453984260559, 0.2502824366092682, 0.6313385963439941] ...\n\t\t [0.7517496943473816, -1.0173239707946777, 0.4567146301269531, 0.770895779132843, -1.0159655809402466]\n\n10 --------------------\npath                      train_landmark_files/37779/1002526690.parquet\nparticipant_id                                                    37779\nsequence_id                                                  1002526690\nsign                                                                bee\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                                19\nfold                                                                  0\nName: 10, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([7, 82, 3])\n\t min/max: -1.0755250453948975 / 1.4256017208099365\n\t is_contiguous: True\n\t values:\n\t\t [0.3668118119239807, 0.6079865097999573, -0.8985936045646667, 0.38047438859939575, 0.6029800176620483] ...\n\t\t [0.6797853112220764, -1.0417932271957397, -0.13120238482952118, 0.6129040122032166, -1.0502601861953735]\n\n11 --------------------\npath                      train_landmark_files/37779/1002801453.parquet\nparticipant_id                                                    37779\nsequence_id                                                  1002801453\nsign                                                           yourself\nlandmark_file_path    /home/user/Data/asl-signs/train_landmark_files...\nnpy_file_path         /home/user/Data/asl-signs/train_features_npy_f...\nlabel                                                               246\nfold                                                                  0\nName: 11, dtype: object \n\nxyz\n\t dtype: torch.float32\n\t shape: torch.Size([7, 82, 3])\n\t min/max: -2.361575126647949 / 1.7345937490463257\n\t is_contiguous: True\n\t values:\n\t\t [0.37138479948043823, 0.5532127618789673, -0.888463020324707, 0.38432031869888306, 0.5442971587181091] ...\n\t\t [1.7210133075714111, -1.8778843879699707, -0.20284615457057953, 1.7345937490463257, -1.9374130964279175]\n\nbatch_size   : 8\nlen(loader)  : 2869\nlen(dataset) : 22959\n\nbatch  0 ===================\nindex [0, 1, 2, 3, 4, 5, 6, 7]\nxyz:\n\t torch.Size([33, 82, 3])\n\t torch.Size([22, 82, 3])\n\t torch.Size([80, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([27, 82, 3])\n\t torch.Size([27, 82, 3])\n\t torch.Size([60, 82, 3])\n\t torch.Size([80, 82, 3])\nlabel:\n\t [231, 195, 49, 207, 14, 142, 62, 0]\n\nbatch  1 ===================\nindex [8, 9, 10, 11, 12, 13, 14, 15]\nxyz:\n\t torch.Size([18, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([7, 82, 3])\n\t torch.Size([7, 82, 3])\n\t torch.Size([23, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([19, 82, 3])\n\t torch.Size([21, 82, 3])\nlabel:\n\t [67, 25, 19, 246, 172, 206, 243, 51]\n\nbatch  2 ===================\nindex [16, 17, 18, 19, 20, 21, 22, 23]\nxyz:\n\t torch.Size([13, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([53, 82, 3])\n\t torch.Size([10, 82, 3])\n\t torch.Size([24, 82, 3])\n\t torch.Size([10, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([55, 82, 3])\nlabel:\n\t [247, 147, 239, 217, 136, 138, 17, 85]\n\nbatch  3 ===================\nindex [24, 25, 26, 27, 28, 29, 30, 31]\nxyz:\n\t torch.Size([10, 82, 3])\n\t torch.Size([8, 82, 3])\n\t torch.Size([9, 82, 3])\n\t torch.Size([12, 82, 3])\n\t torch.Size([26, 82, 3])\n\t torch.Size([3, 82, 3])\n\t torch.Size([80, 82, 3])\n\t torch.Size([11, 82, 3])\nlabel:\n\t [115, 220, 61, 209, 110, 73, 45, 90]\n\nbatch  4 ===================\nindex [32, 33, 34, 35, 36, 37, 38, 39]\nxyz:\n\t torch.Size([45, 82, 3])\n\t torch.Size([14, 82, 3])\n\t torch.Size([3, 82, 3])\n\t torch.Size([24, 82, 3])\n\t torch.Size([13, 82, 3])\n\t torch.Size([45, 82, 3])\n\t torch.Size([8, 82, 3])\n\t torch.Size([25, 82, 3])\nlabel:\n\t [165, 23, 238, 117, 195, 67, 223, 4]\n\nbatch  5 ===================\nindex [40, 41, 42, 43, 44, 45, 46, 47]\nxyz:\n\t torch.Size([8, 82, 3])\n\t torch.Size([48, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([6, 82, 3])\n\t torch.Size([9, 82, 3])\n\t torch.Size([80, 82, 3])\n\t torch.Size([34, 82, 3])\n\t torch.Size([18, 82, 3])\nlabel:\n\t [52, 75, 77, 93, 200, 226, 60, 65]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{"papermill":{"duration":0.007117,"end_time":"2023-06-03T09:29:02.695478","exception":false,"start_time":"2023-06-03T09:29:02.688361","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_VISIBLE_DEVICES']='0'","metadata":{"papermill":{"duration":0.016164,"end_time":"2023-06-03T09:29:02.718989","exception":false,"start_time":"2023-06-03T09:29:02.702825","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:51.784995Z","iopub.execute_input":"2023-06-04T19:59:51.785336Z","iopub.status.idle":"2023-06-04T19:59:51.789451Z","shell.execute_reply.started":"2023-06-04T19:59:51.785305Z","shell.execute_reply":"2023-06-04T19:59:51.788541Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# additional helper functions 2\n#assum zero-mean one-std, input\ndef do_random_affine(xyz, scale=(0.8, 1.5), shift=(-0.1, 0.1), degree=(-15, 15), p=0.5):\n    transformed_xyz = np.copy(xyz)  # Create a copy of xyz to store transformed values\n\n    if np.random.rand() < p:\n        if scale is not None:\n            scale_factor = np.random.uniform(*scale)\n            transformed_xyz = scale_factor * transformed_xyz\n\n        if shift is not None:\n            shift_value = np.random.uniform(*shift)\n            transformed_xyz = transformed_xyz + shift_value\n\n        if degree is not None:\n            degree_value = np.random.uniform(*degree)\n            radian = degree_value / 180 * np.pi\n            c = np.cos(radian)\n            s = np.sin(radian)\n            rotate = np.array([[c, -s], [s, c]]).T\n            transformed_xyz[..., :2] = transformed_xyz[..., :2] @ rotate\n\n    return transformed_xyz\n            \ndef get_learning_rate(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\ndef time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n\n    else:\n        raise NotImplementedError\n","metadata":{"papermill":{"duration":0.021073,"end_time":"2023-06-03T09:29:02.747368","exception":false,"start_time":"2023-06-03T09:29:02.726295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:51.790810Z","iopub.execute_input":"2023-06-04T19:59:51.791357Z","iopub.status.idle":"2023-06-04T19:59:51.803594Z","shell.execute_reply.started":"2023-06-04T19:59:51.791314Z","shell.execute_reply":"2023-06-04T19:59:51.802774Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### RAdam and Lookahead","metadata":{"papermill":{"duration":0.00712,"end_time":"2023-06-03T09:29:02.761773","exception":false,"start_time":"2023-06-03T09:29:02.754653","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Optimizer\nfrom collections import defaultdict\nimport math\nfrom timeit import default_timer as timer","metadata":{"papermill":{"duration":0.015212,"end_time":"2023-06-03T09:29:02.784137","exception":false,"start_time":"2023-06-03T09:29:02.768925","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:51.806468Z","iopub.execute_input":"2023-06-04T19:59:51.806850Z","iopub.status.idle":"2023-06-04T19:59:51.818572Z","shell.execute_reply.started":"2023-06-04T19:59:51.806819Z","shell.execute_reply":"2023-06-04T19:59:51.817721Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def np_cross_entropy(probability, truth):\n    p = np.clip(probability,1e-4,1-1e-4)\n    logp = -np.log(p)\n    loss = logp[np.arange(len(logp)),truth]\n    loss = loss.mean()\n    return loss","metadata":{"papermill":{"duration":0.015262,"end_time":"2023-06-03T09:29:02.806841","exception":false,"start_time":"2023-06-03T09:29:02.791579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:51.821950Z","iopub.execute_input":"2023-06-04T19:59:51.822223Z","iopub.status.idle":"2023-06-04T19:59:51.830000Z","shell.execute_reply.started":"2023-06-04T19:59:51.822200Z","shell.execute_reply":"2023-06-04T19:59:51.829179Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom tqdm import tqdm\n\nfold_type = 'christ-part'\nfold     = 2\nout_dir  = '/kaggle/input' + '/result/run10/transfomer-80-256-lip-hand-3a'\nfold_dir = out_dir+ f'/fold-{fold}-{fold_type}'\n\nstart_lr = 1e-4\nbatch_size = 64\nnum_epoch = 16\nsave_iter = 2\n\ntrain_df, valid_df = read_christ_csv_by_part(fold)\ntrain_dataset = SignDataset(train_df,train_augment)\nvalid_dataset = SignDataset(valid_df,)\n\ntrain_loader  = DataLoader(\n        train_dataset,\n        sampler = RandomSampler(train_dataset),\n        #sampler = BalanceSampler(train_dataset),\n        batch_size  = batch_size,\n        drop_last   = True,\n        num_workers = 2,\n        pin_memory  = False,\n        worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n        collate_fn = null_collate,\n    )\n\nvalid_loader = DataLoader(\n        valid_dataset,\n        sampler = SequentialSampler(valid_dataset),\n        batch_size  = 64,\n        drop_last   = False,\n        num_workers = 2,\n        pin_memory  = False,\n        collate_fn = null_collate,\n    )\n\nnet = Net()\n#net = nn.DataParallel(net)\nscaler = torch.cuda.amp.GradScaler(enabled = True) # To scale the extremely small gradients\nnet.to(device)\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n","metadata":{"papermill":{"duration":3.563454,"end_time":"2023-06-03T09:29:06.377514","exception":false,"start_time":"2023-06-03T09:29:02.814060","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:51.832590Z","iopub.execute_input":"2023-06-04T19:59:51.833633Z","iopub.status.idle":"2023-06-04T19:59:55.696161Z","shell.execute_reply.started":"2023-06-04T19:59:51.833603Z","shell.execute_reply":"2023-06-04T19:59:55.695202Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Load the model from a checkpoint: ","metadata":{"papermill":{"duration":0.007192,"end_time":"2023-06-03T09:29:06.393269","exception":false,"start_time":"2023-06-03T09:29:06.386077","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_checkpoint():\n    start_epoch = 0\n    initial_checkpoint = r\"/kaggle/input/islr-self/00000030.model.pth\"\n    f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n    start_epoch = f.get('epoch',0)+1\n    state_dict = f['state_dict']\n    net.load_state_dict(state_dict,strict=False)\n    optimizer.load_state_dict(f['optimizer_dict'])\n    scaler.load_state_dict(f['scheduler_dict'])\n    print(f\"Starting from Epoch: {start_epoch}\")\n\n# if __name__==\"main\":\n#     load_checkpoint()","metadata":{"papermill":{"duration":0.017597,"end_time":"2023-06-03T09:29:06.418035","exception":false,"start_time":"2023-06-03T09:29:06.400438","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:55.699199Z","iopub.execute_input":"2023-06-04T19:59:55.700914Z","iopub.status.idle":"2023-06-04T19:59:55.707973Z","shell.execute_reply.started":"2023-06-04T19:59:55.700870Z","shell.execute_reply":"2023-06-04T19:59:55.706472Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def run_train():\n    for i in tqdm(range(start_epoch , num_epoch+start_epoch)): # change to num_epochs\n        net.train()\n        train_sign = []\n        train_truth = []\n        train_num = 0\n        train_loss_sum = 0\n        train_total = 0\n        for t, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n            rate = get_learning_rate(optimizer)\n\n            with torch.cuda.amp.autocast(enabled = True):\n                batch_size = len(batch['index'])\n                batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n                batch['label'] = batch['label'].cuda()\n                #y = batch['label']\n                #x = batch['xyz']\n                net.output_type = ['loss', 'inference']\n                output = net(batch)\n                loss0  = output['label_loss'].mean()\n\n\n            train_sign.append(output['sign'].detach().cpu().numpy())\n            train_truth.append(batch['label'].detach().cpu().numpy())\n            train_num += batch_size\n\n            optimizer.zero_grad()\n            scaler.scale(loss0).backward()\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss_sum += loss0.item()\n            train_total += 1\n\n        train_truth = np.concatenate(train_truth)\n        sign = np.concatenate(train_sign)\n        topk_indices = np.argsort(sign, -1)[:,-5:]\n        correct_topk = np.any(np.equal(topk_indices, train_truth.reshape(-1, 1)), axis=1)\n        topk_accuracy_train = np.mean(correct_topk)\n\n        val_loss_sum = 0\n        val_total = 0\n        valid_num = 0\n        valid_sign=[]\n        net.eval()\n\n        for t, batch in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n\n            net.output_type = ['inference']\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled = True):\n                    batch_size = len(batch['index'])\n                    batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n                    batch['label'] = batch['label'].cuda()\n                    output = net(batch)\n                    valid_loss = np_cross_entropy(output['sign'].detach().cpu(), batch['label'].detach().cpu())\n                    val_loss_sum += valid_loss\n                    val_total +=1\n                valid_sign.append(output['sign'].detach().cpu().numpy())\n                valid_num += batch_size\n\n        valid_truth = valid_loader.dataset.df.label.values\n        sign = np.concatenate(valid_sign)\n        topk_indices = np.argsort(sign, -1)[:,-5:]\n        correct_topk = np.any(np.equal(topk_indices, valid_truth.reshape(-1, 1)), axis=1)\n        topk_accuracy_valid = np.mean(correct_topk)\n\n        print(f\"Epoch:{i} => Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {topk_accuracy_train:0.04f}\")\n        print(f\"Epoch:{i} => Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {topk_accuracy_valid:0.04f}\")\n        print(\"=\"*50)\n\n        if i%save_iter==0:\n            if i != start_epoch:\n                n = i\n                torch.save({\n                           'state_dict': net.state_dict(),\n                            'epoch': i,\n                            'optimizer_dict': optimizer.state_dict(),\n                            'scheduler_dict':scaler.state_dict(),\n                            }, f'{n:08d}.model.pth')\n        torch.cuda.empty_cache()\n        \n# if __name__ == \"__main__\":\n#     run_train()","metadata":{"papermill":{"duration":0.028198,"end_time":"2023-06-03T09:29:06.453481","exception":false,"start_time":"2023-06-03T09:29:06.425283","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T19:59:55.709654Z","iopub.execute_input":"2023-06-04T19:59:55.710040Z","iopub.status.idle":"2023-06-04T19:59:55.731090Z","shell.execute_reply.started":"2023-06-04T19:59:55.710005Z","shell.execute_reply":"2023-06-04T19:59:55.730202Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"net = Net()\nnet.load_state_dict(torch.load('/kaggle/input/gislr-saved-models/00000038.model.pth', map_location= torch.device('cuda'))['state_dict'])","metadata":{"papermill":{"duration":0.227519,"end_time":"2023-06-03T09:29:06.688096","exception":false,"start_time":"2023-06-03T09:29:06.460577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T20:00:31.070950Z","iopub.execute_input":"2023-06-04T20:00:31.071813Z","iopub.status.idle":"2023-06-04T20:00:31.136126Z","shell.execute_reply.started":"2023-06-04T20:00:31.071781Z","shell.execute_reply":"2023-06-04T20:00:31.135106Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference_on_random_example():\n    kaggle_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv')\n    randno = np.random.randint(len(kaggle_df))\n    valid_df = kaggle_df[kaggle_df.index==randno].reset_index(drop=True)\n    valid_dataset = SignDataset(valid_df,)\n    print(\"Actual label: \",valid_df.iloc[0]['label'])\n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler = SequentialSampler(valid_dataset),\n        batch_size  = 1,\n        drop_last   = False,\n        num_workers = 2,\n        pin_memory  = False,\n        collate_fn = null_collate,\n    )\n    \n    for t, batch in enumerate(valid_loader):\n        net.output_type = ['inference']\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled = True):\n                output = net(batch)\n                top_values, top_indices = torch.topk(output['sign'].detach().cpu(), k=5)\n                print(top_indices)\n                print(top_values)\n\nif __name__==\"__main__\":\n    run_inference_on_random_example()","metadata":{"papermill":{"duration":0.494408,"end_time":"2023-06-03T09:29:07.190377","exception":false,"start_time":"2023-06-03T09:29:06.695969","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T20:00:38.942423Z","iopub.execute_input":"2023-06-04T20:00:38.942781Z","iopub.status.idle":"2023-06-04T20:00:39.491188Z","shell.execute_reply.started":"2023-06-04T20:00:38.942754Z","shell.execute_reply":"2023-06-04T20:00:39.489295Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Actual label:  53\ntensor([[ 53, 192, 180,  74, 184]])\ntensor([[0.7807, 0.0864, 0.0636, 0.0144, 0.0139]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Inference on a single file (dataframe)**","metadata":{"papermill":{"duration":0.007388,"end_time":"2023-06-03T09:29:07.205906","exception":false,"start_time":"2023-06-03T09:29:07.198518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class InferenceDataset(Dataset):\n    def __init__(self, csv_path):\n        self.csv_path = csv_path\n        self.length = 1  # Set length to 1 for inference on a single file\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        xyz = load_relevant_data_subset(self.csv_path, type='csv')\n        xyz = xyz - xyz[~np.isnan(xyz)].mean(0, keepdims=True)  # Normalization to common mean\n        xyz = xyz / xyz[~np.isnan(xyz)].std(0, keepdims=True)\n        xyz = torch.from_numpy(xyz).float()\n        xyz = pre_process(xyz)\n\n        r = {}\n        r['xyz'] = xyz\n        return r","metadata":{"papermill":{"duration":0.018626,"end_time":"2023-06-03T09:29:07.231945","exception":false,"start_time":"2023-06-03T09:29:07.213319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T20:00:45.084200Z","iopub.execute_input":"2023-06-04T20:00:45.084568Z","iopub.status.idle":"2023-06-04T20:00:45.094382Z","shell.execute_reply.started":"2023-06-04T20:00:45.084537Z","shell.execute_reply":"2023-06-04T20:00:45.093319Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sign_path = '/kaggle/input/blow-dataset/cat.csv'\npd.read_csv('/kaggle/input/blow-dataset/cat.csv').head()","metadata":{"papermill":{"duration":0.080575,"end_time":"2023-06-03T09:29:07.319887","exception":false,"start_time":"2023-06-03T09:29:07.239312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T20:00:46.342711Z","iopub.execute_input":"2023-06-04T20:00:46.343067Z","iopub.status.idle":"2023-06-04T20:00:46.436195Z","shell.execute_reply.started":"2023-06-04T20:00:46.343042Z","shell.execute_reply":"2023-06-04T20:00:46.435255Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"          x         y         z\n0  0.436362  0.356953 -0.048455\n1  0.430710  0.321346 -0.086460\n2  0.433086  0.333287 -0.047363\n3  0.413351  0.289695 -0.062578\n4  0.429143  0.310761 -0.091302","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.436362</td>\n      <td>0.356953</td>\n      <td>-0.048455</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.430710</td>\n      <td>0.321346</td>\n      <td>-0.086460</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.433086</td>\n      <td>0.333287</td>\n      <td>-0.047363</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.413351</td>\n      <td>0.289695</td>\n      <td>-0.062578</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.429143</td>\n      <td>0.310761</td>\n      <td>-0.091302</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"json_file_path = '/kaggle/input/asl-signs/sign_to_prediction_index_map.json'\nwith open(json_file_path, 'r') as json_file:\n    sign2label = json.load(json_file)\n    \nvd = InferenceDataset(sign_path,)\nprint(vd)\nvalid_loader = DataLoader(\n    vd,\n    batch_size  = 1,\n    shuffle=False,\n    num_workers = 0,\n)\nfor t, batch in enumerate(valid_loader):\n    net.output_type = ['inference']\n    with torch.no_grad():\n        with torch.cuda.amp.autocast(enabled = True):\n            output = net(batch)\n            top_values, top_indices = torch.topk(output['sign'].detach().cpu(), k=5)\n            print(top_indices)\n            print(top_values)\n            \n            for value in top_indices[0]:\n                corresponding_key = None\n                for key, json_value in sign2label.items():\n                    if json_value == value:\n                        corresponding_key = key\n                        break\n                        \n                print(f\"Sign: {corresponding_key}\")","metadata":{"papermill":{"duration":0.098709,"end_time":"2023-06-03T09:29:07.426660","exception":false,"start_time":"2023-06-03T09:29:07.327951","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-04T20:00:49.255830Z","iopub.execute_input":"2023-06-04T20:00:49.256696Z","iopub.status.idle":"2023-06-04T20:00:49.321442Z","shell.execute_reply.started":"2023-06-04T20:00:49.256631Z","shell.execute_reply":"2023-06-04T20:00:49.320435Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<__main__.InferenceDataset object at 0x7b254a5f7700>\ntensor([[ 38, 129,  78, 104, 249]])\ntensor([[0.6334, 0.2779, 0.0526, 0.0098, 0.0057]])\nSign: cat\nSign: kitty\nSign: find\nSign: hair\nSign: zipper\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import HTML\n\nHTML(\"<h1 style='background-color: #FFEBBA; color: #1e1e1e; font-size: 32px; font-family: garamond; padding:12px 50%'>INCLUDE DATASET</h1>\")","metadata":{"papermill":{"duration":0.007539,"end_time":"2023-06-03T09:29:07.442383","exception":false,"start_time":"2023-06-03T09:29:07.434844","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-04T20:00:51.691192Z","iopub.execute_input":"2023-06-04T20:00:51.691542Z","iopub.status.idle":"2023-06-04T20:00:51.699147Z","shell.execute_reply.started":"2023-06-04T20:00:51.691516Z","shell.execute_reply":"2023-06-04T20:00:51.697952Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h1 style='background-color: #FFEBBA; color: #1e1e1e; font-size: 32px; font-family: garamond; padding:12px 50%'>INCLUDE DATASET</h1>"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/include-dataset/train-preprocessed.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:00:53.067740Z","iopub.execute_input":"2023-06-04T20:00:53.068087Z","iopub.status.idle":"2023-06-04T20:00:53.088155Z","shell.execute_reply.started":"2023-06-04T20:00:53.068061Z","shell.execute_reply":"2023-06-04T20:00:53.087083Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                                path sign  category  label\n0  /kaggle/input/include-dataset/islr-xyz/islr-xy...    I  Pronouns     19\n1  /kaggle/input/include-dataset/islr-xyz/islr-xy...    I  Pronouns     19\n2  /kaggle/input/include-dataset/islr-xyz/islr-xy...    I  Pronouns     19\n3  /kaggle/input/include-dataset/islr-xyz/islr-xy...    I  Pronouns     19\n4  /kaggle/input/include-dataset/islr-xyz/islr-xy...    I  Pronouns     19","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>sign</th>\n      <th>category</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/include-dataset/islr-xyz/islr-xy...</td>\n      <td>I</td>\n      <td>Pronouns</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/include-dataset/islr-xyz/islr-xy...</td>\n      <td>I</td>\n      <td>Pronouns</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/include-dataset/islr-xyz/islr-xy...</td>\n      <td>I</td>\n      <td>Pronouns</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/include-dataset/islr-xyz/islr-xy...</td>\n      <td>I</td>\n      <td>Pronouns</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/include-dataset/islr-xyz/islr-xy...</td>\n      <td>I</td>\n      <td>Pronouns</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def read_csvs_by_fold(fold=0):\n    num_fold = 5\n    data = pd.read_csv('/kaggle/input/include-dataset/train-preprocessed.csv')\n    data.loc[:, 'fold' ] = -1\n    sgkf = StratifiedGroupKFold(n_splits=num_fold, random_state=123, shuffle=True)\n    for i, (train_index, valid_index) in enumerate(sgkf.split(data[['path', 'sign']], data.label, data.category)):\n        data.loc[valid_index,'fold'] = i\n    train_df = data[data.fold!=fold].reset_index(drop=True)\n    valid_df = data[data.fold==fold].reset_index(drop=True)\n    return train_df, valid_df","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:00:54.560228Z","iopub.execute_input":"2023-06-04T20:00:54.560595Z","iopub.status.idle":"2023-06-04T20:00:54.568050Z","shell.execute_reply.started":"2023-06-04T20:00:54.560566Z","shell.execute_reply":"2023-06-04T20:00:54.566949Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class IncludeDataset(Dataset):\n    def __init__(self, df, augment=None):\n        self.df = df\n        self.augment = augment\n        self.length = len(self.df)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n\n        csv_file = d.path\n        xyz = load_relevant_data_subset(csv_file, type=\"csv\")\n        xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n        xyz = xyz / xyz[~np.isnan(xyz)].std(0, keepdims=True)\n\n        if self.augment is not None:\n            xyz = self.augment(xyz)\n        xyz = torch.from_numpy(xyz).float()\n        xyz = pre_process(xyz)\n\n        r = {}\n        r['index'] = index\n        r['d'    ] = d\n        r['xyz'  ] = xyz\n        r['label'] = d.label\n        r['category'] = d.category\n        return r","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:00:56.688989Z","iopub.execute_input":"2023-06-04T20:00:56.689338Z","iopub.status.idle":"2023-06-04T20:00:56.699729Z","shell.execute_reply.started":"2023-06-04T20:00:56.689310Z","shell.execute_reply":"2023-06-04T20:00:56.697722Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def run_check_dataset():\n    train_df, valid_df = read_csvs_by_fold(fold=0)\n    dataset = IncludeDataset(valid_df)\n\n    loader = DataLoader(\n        dataset,\n        sampler=SequentialSampler(dataset),\n        batch_size=8,\n        drop_last=True,\n        num_workers=0,\n        pin_memory=False,\n        worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n        collate_fn=null_collate,\n    )\n    for t, batch in enumerate(loader):\n        if t > 0: break\n        print(batch['index'])\n\nif __name__ == '__main__':\n    run_check_dataset()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:00:58.051222Z","iopub.execute_input":"2023-06-04T20:00:58.051622Z","iopub.status.idle":"2023-06-04T20:00:58.811023Z","shell.execute_reply.started":"2023-06-04T20:00:58.051592Z","shell.execute_reply":"2023-06-04T20:00:58.810051Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[0, 1, 2, 3, 4, 5, 6, 7]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2 style=\"color:red; font-weight:600; font-family: badoni;\">Dataloaders</h2>\n<p style=\"color:green; font-weight:300; font-family:verdana;\">Common mistake : 1. Not changing \"SignDataset\"->\"IncludeDataset\" 2. \"net\"->\"net_plus</p>","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LambdaLR\nfold     = 2\nout_dir  = '/kaggle/input' + '/result/run10/transfomer-80-256-lip-hand-3a'\nfold_dir = out_dir+ f'/fold-{fold}-{fold_type}'\n\nstart_lr = 5e-5 # 1e-4\nbatch_size = 64\nnum_epoch = 16\nsave_iter = 2\n\ntrain_df, valid_df = read_csvs_by_fold(fold)\ntrain_dataset = IncludeDataset(train_df,train_augment)\nvalid_dataset = IncludeDataset(valid_df,)\n\ntrain_loader  = DataLoader(\n        train_dataset,\n        sampler = RandomSampler(train_dataset),\n        batch_size  = batch_size,\n        drop_last   = True,\n        num_workers = 2,\n        pin_memory  = False,\n        worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n        collate_fn = null_collate,\n    )\n\nvalid_loader = DataLoader(\n        valid_dataset,\n        sampler = SequentialSampler(valid_dataset),\n        batch_size  = 64,\n        drop_last   = False,\n        num_workers = 2,\n        pin_memory  = False,\n        collate_fn = null_collate,\n    )\n\nnet_plus = Net(num_class=64)\nscaler = torch.cuda.amp.GradScaler(enabled = True) # To scale the extremely small gradients\nnet_plus.to(device)\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net_plus.parameters()),lr=start_lr)\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: epoch / num_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:01:02.523520Z","iopub.execute_input":"2023-06-04T20:01:02.524061Z","iopub.status.idle":"2023-06-04T20:01:02.592882Z","shell.execute_reply.started":"2023-06-04T20:01:02.524021Z","shell.execute_reply":"2023-06-04T20:01:02.591964Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"state_dict = torch.load(\"/kaggle/input/gislr-saved-models/00000038.model.pth\", map_location=torch.device('cuda'))['state_dict']\nstate_dict = {k: v for k, v in state_dict.items() if not k.startswith('logit')}\nnet_plus.load_state_dict(state_dict, strict=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:01:11.364499Z","iopub.execute_input":"2023-06-04T20:01:11.364864Z","iopub.status.idle":"2023-06-04T20:01:11.402554Z","shell.execute_reply.started":"2023-06-04T20:01:11.364837Z","shell.execute_reply":"2023-06-04T20:01:11.401663Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"_IncompatibleKeys(missing_keys=['logit.weight', 'logit.bias'], unexpected_keys=[])"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2 style=\"color:red; font-weight:600; font-family: badoni;\">Load the model from check point</h2>","metadata":{}},{"cell_type":"code","source":"def load_checkpoint():\n    start_epoch = 0\n    initial_checkpoint = r\"/kaggle/input/islr-self/lasilfjiogitoiet\"\n    f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n    start_epoch = f.get('epoch',0)+1\n    state_dict = f['state_dict']\n    net_plus.load_state_dict(state_dict,strict=False)\n    optimizer.load_state_dict(f['optimizer_dict'])\n    scaler.load_state_dict(f['scheduler_dict'])\n    print(f\"Starting from Epoch: {start_epoch}\")\n\n# if __name__==\"main\":\n#     load_checkpoint()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:01:13.694210Z","iopub.execute_input":"2023-06-04T20:01:13.694566Z","iopub.status.idle":"2023-06-04T20:01:13.701189Z","shell.execute_reply.started":"2023-06-04T20:01:13.694539Z","shell.execute_reply":"2023-06-04T20:01:13.700004Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color:green; font-weight:300; font-family:verdana; padding-left:20px; font-size:40px\">Run train</p>","metadata":{}},{"cell_type":"code","source":"# Freezing the weights of the layers\nfor name, param in net_plus.named_parameters():\n    if not name.startswith('logit'):\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:01:19.892155Z","iopub.execute_input":"2023-06-04T20:01:19.892494Z","iopub.status.idle":"2023-06-04T20:01:19.897801Z","shell.execute_reply.started":"2023-06-04T20:01:19.892469Z","shell.execute_reply":"2023-06-04T20:01:19.896636Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"start_epoch=0\ndef run_train():\n    for i in tqdm(range(start_epoch , num_epoch+start_epoch)): # change to num_epochs\n        net_plus.train()\n        train_sign = []\n        train_truth = []\n        train_num = 0\n        train_loss_sum = 0\n        train_total = 0\n        for t, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n            rate = get_learning_rate(optimizer)\n\n            with torch.cuda.amp.autocast(enabled = True):\n                batch_size = len(batch['index'])\n                batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n                batch['label'] = batch['label'].cuda()\n                net_plus.output_type = ['loss', 'inference']\n                output = net_plus(batch)\n                loss0  = output['label_loss'].mean()\n\n\n            train_sign.append(output['sign'].detach().cpu().numpy())\n            train_truth.append(batch['label'].detach().cpu().numpy())\n            train_num += batch_size\n            \n            \n            optimizer.zero_grad()\n            scaler.scale(loss0).backward()\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss_sum += loss0.item()\n            train_total += 1\n\n        scheduler.step()\n        train_truth = np.concatenate(train_truth)\n        sign = np.concatenate(train_sign)\n        topk_indices = np.argsort(sign, -1)[:,-5:]\n        correct_topk = np.any(np.equal(topk_indices, train_truth.reshape(-1, 1)), axis=1)\n        topk_accuracy_train = np.mean(correct_topk)\n\n        val_loss_sum = 0\n        val_total = 0\n        valid_num = 0\n        valid_sign=[]\n        net_plus.eval()\n\n        for t, batch in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n\n            net_plus.output_type = ['inference']\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled = True):\n                    batch_size = len(batch['index'])\n                    batch['xyz'] = [xyz.cuda() for xyz in batch['xyz']]\n                    batch['label'] = batch['label'].cuda()\n                    output = net_plus(batch)\n                    valid_loss = np_cross_entropy(output['sign'].detach().cpu(), batch['label'].detach().cpu())\n                    val_loss_sum += valid_loss\n                    val_total +=1\n                valid_sign.append(output['sign'].detach().cpu().numpy())\n                valid_num += batch_size\n\n        valid_truth = valid_loader.dataset.df.label.values\n        sign = np.concatenate(valid_sign)\n        topk_indices = np.argsort(sign, -1)[:,-5:]\n        correct_topk = np.any(np.equal(topk_indices, valid_truth.reshape(-1, 1)), axis=1)\n        topk_accuracy_valid = np.mean(correct_topk)\n\n        print(f\"Epoch:{i} => Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {topk_accuracy_train:0.04f}\")\n        print(f\"Epoch:{i} => Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {topk_accuracy_valid:0.04f}\")\n        print(\"=\"*50)\n\n        if i%save_iter==0:\n            if i != start_epoch:\n                n = i\n                torch.save({\n                           'state_dict': net_plus.state_dict(),\n                            'epoch': i,\n                            'optimizer_dict': optimizer.state_dict(),\n                            'scheduler_dict':scaler.state_dict(),\n                            }, f'{n:08d}.model.pth')\n        torch.cuda.empty_cache()\n        \nif __name__ == \"__main__\":\n    run_train()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:01:25.426934Z","iopub.execute_input":"2023-06-04T20:01:25.427278Z","iopub.status.idle":"2023-06-04T20:07:41.521254Z","shell.execute_reply.started":"2023-06-04T20:01:25.427250Z","shell.execute_reply":"2023-06-04T20:07:41.519985Z"},"scrolled":true,"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"  0%|          | 0/16 [00:00<?, ?it/s]\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:06<01:12,  6.56s/it]\u001b[A\n 25%|       | 3/12 [00:08<00:21,  2.38s/it]\u001b[A\n 42%|     | 5/12 [00:12<00:14,  2.12s/it]\u001b[A\n 58%|    | 7/12 [00:15<00:09,  1.94s/it]\u001b[A\n 75%|  | 9/12 [00:19<00:05,  1.86s/it]\u001b[A\n100%|| 12/12 [00:22<00:00,  1.85s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:04<00:23,  4.76s/it]\u001b[A\n 50%|     | 3/6 [00:08<00:07,  2.47s/it]\u001b[A\n100%|| 6/6 [00:10<00:00,  1.78s/it]\u001b[A\n  6%|         | 1/16 [00:32<08:14, 32.97s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:0 => Train Loss: 4.8194, Train Acc: 0.0768\nEpoch:0 => Val Loss: 4.7308, Val Acc: 0.0278\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:03<00:36,  3.33s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:16,  1.82s/it]\u001b[A\n 42%|     | 5/12 [00:08<00:10,  1.52s/it]\u001b[A\n 50%|     | 6/12 [00:08<00:06,  1.14s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 58%|    | 7/12 [00:10<00:07,  1.50s/it]\u001b[A\n 75%|  | 9/12 [00:13<00:04,  1.38s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.32s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.17s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.76s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.21s/it]\u001b[A\n 12%|        | 2/16 [00:56<06:21, 27.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:1 => Train Loss: 4.7879, Train Acc: 0.0664\nEpoch:1 => Val Loss: 4.7387, Val Acc: 0.0278\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:28,  2.59s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:11,  1.14s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:15,  1.70s/it]\u001b[A\n 33%|      | 4/12 [00:05<00:08,  1.11s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.54s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.09s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.44s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.06s/it]\u001b[A\n 75%|  | 9/12 [00:13<00:04,  1.61s/it]\u001b[A\n 83%| | 10/12 [00:13<00:02,  1.21s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 92%|| 11/12 [00:15<00:01,  1.49s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.32s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.18s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.69s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.17s/it]\u001b[A\n 19%|        | 3/16 [01:19<05:29, 25.35s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:2 => Train Loss: 4.8019, Train Acc: 0.0716\nEpoch:2 => Val Loss: 4.7544, Val Acc: 0.0278\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:27,  2.47s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:12,  1.26s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:16,  1.78s/it]\u001b[A\n 33%|      | 4/12 [00:05<00:09,  1.21s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.54s/it]\u001b[A\n 50%|     | 6/12 [00:08<00:06,  1.11s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:07,  1.44s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.03s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.41s/it]\u001b[A\n 83%| | 10/12 [00:12<00:02,  1.04s/it]\u001b[A\n 92%|| 11/12 [00:15<00:01,  1.41s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.27s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.13s/it]\u001b[A\n 50%|     | 3/6 [00:06<00:05,  1.95s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.30s/it]\u001b[A\n 25%|       | 4/16 [01:42<04:54, 24.50s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:3 => Train Loss: 4.8102, Train Acc: 0.0638\nEpoch:3 => Val Loss: 4.7788, Val Acc: 0.0278\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:30,  2.75s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:12,  1.25s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:14,  1.66s/it]\u001b[A\n 33%|      | 4/12 [00:05<00:08,  1.11s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.44s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.07s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.40s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.10s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 75%|  | 9/12 [00:12<00:04,  1.40s/it]\u001b[A\n 83%| | 10/12 [00:12<00:02,  1.07s/it]\u001b[A\n 92%|| 11/12 [00:14<00:01,  1.43s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.26s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.09s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.71s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.18s/it]\u001b[A\n 31%|      | 5/16 [02:04<04:20, 23.72s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:4 => Train Loss: 4.7745, Train Acc: 0.0716\nEpoch:4 => Val Loss: 4.8120, Val Acc: 0.0247\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:28,  2.55s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:14,  1.59s/it]\u001b[A\n 42%|     | 5/12 [00:08<00:11,  1.60s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:07,  1.46s/it]\u001b[A\n 75%|  | 9/12 [00:13<00:04,  1.40s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.31s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:16,  3.23s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.69s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.18s/it]\u001b[A\n 38%|      | 6/16 [02:27<03:54, 23.47s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:5 => Train Loss: 4.6993, Train Acc: 0.0677\nEpoch:5 => Val Loss: 4.8539, Val Acc: 0.0216\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n  8%|         | 1/12 [00:02<00:29,  2.69s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:14,  1.57s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.45s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:06,  1.36s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:03,  1.32s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.25s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:19,  3.85s/it]\u001b[A\n 50%|     | 3/6 [00:06<00:05,  1.88s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.30s/it]\u001b[A\n 44%|     | 7/16 [02:50<03:29, 23.31s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:6 => Train Loss: 4.6228, Train Acc: 0.0924\nEpoch:6 => Val Loss: 4.9047, Val Acc: 0.0216\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:30,  2.81s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:14,  1.62s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:09,  1.43s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 58%|    | 7/12 [00:10<00:07,  1.44s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.33s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.27s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:14,  3.00s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.67s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.20s/it]\u001b[A\n 50%|     | 8/16 [03:13<03:04, 23.08s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:7 => Train Loss: 4.5647, Train Acc: 0.0938\nEpoch:7 => Val Loss: 4.9644, Val Acc: 0.0216\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:03<00:35,  3.24s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 25%|       | 3/12 [00:05<00:15,  1.76s/it]\u001b[A\n 42%|     | 5/12 [00:08<00:10,  1.46s/it]\u001b[A\n 50%|     | 6/12 [00:08<00:06,  1.11s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:07,  1.50s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.14s/it]\u001b[A\n 75%|  | 9/12 [00:13<00:04,  1.44s/it]\u001b[A\n 83%| | 10/12 [00:13<00:02,  1.12s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.31s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.12s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.76s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.21s/it]\u001b[A\n 56%|    | 9/16 [03:36<02:41, 23.10s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:8 => Train Loss: 4.4763, Train Acc: 0.1003\nEpoch:8 => Val Loss: 5.0325, Val Acc: 0.0185\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:26,  2.42s/it]\u001b[A\n 25%|       | 3/12 [00:04<00:13,  1.49s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:09,  1.38s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.05s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.44s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.06s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.62s/it]\u001b[A\n 83%| | 10/12 [00:13<00:02,  1.21s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n100%|| 12/12 [00:15<00:00,  1.30s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.20s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.75s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.20s/it]\u001b[A\n 62%|   | 10/16 [03:59<02:18, 23.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:9 => Train Loss: 4.4510, Train Acc: 0.1146\nEpoch:9 => Val Loss: 5.1071, Val Acc: 0.0154\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:27,  2.46s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:10,  1.08s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:15,  1.70s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.43s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.05s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.44s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.08s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.38s/it]\u001b[A\n 83%| | 10/12 [00:12<00:02,  1.09s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 92%|| 11/12 [00:14<00:01,  1.33s/it]\u001b[A\n100%|| 12/12 [00:14<00:00,  1.25s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.06s/it]\u001b[A\n 50%|     | 3/6 [00:06<00:05,  1.88s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.29s/it]\u001b[A\n 69%|   | 11/16 [04:22<01:54, 22.99s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:10 => Train Loss: 4.2759, Train Acc: 0.1354\nEpoch:10 => Val Loss: 5.1904, Val Acc: 0.0123\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:28,  2.60s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:11,  1.14s/it]\u001b[A\n 25%|       | 3/12 [00:04<00:14,  1.63s/it]\u001b[A\n 33%|      | 4/12 [00:05<00:08,  1.07s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.49s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.05s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.45s/it]\u001b[A\n 67%|   | 8/12 [00:09<00:04,  1.02s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 75%|  | 9/12 [00:12<00:04,  1.47s/it]\u001b[A\n100%|| 12/12 [00:14<00:00,  1.24s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.09s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.72s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.19s/it]\u001b[A\n 75%|  | 12/16 [04:44<01:30, 22.73s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:11 => Train Loss: 4.2259, Train Acc: 0.1484\nEpoch:11 => Val Loss: 5.2788, Val Acc: 0.0123\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:28,  2.59s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 25%|       | 3/12 [00:05<00:14,  1.58s/it]\u001b[A\n 33%|      | 4/12 [00:05<00:08,  1.07s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.57s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.14s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:08,  1.64s/it]\u001b[A\n 67%|   | 8/12 [00:11<00:04,  1.25s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.45s/it]\u001b[A\n 83%| | 10/12 [00:13<00:02,  1.14s/it]\u001b[A\n 92%|| 11/12 [00:15<00:01,  1.38s/it]\u001b[A\n100%|| 12/12 [00:15<00:00,  1.30s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:16,  3.32s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.73s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.21s/it]\u001b[A\n 81%| | 13/16 [05:07<01:08, 22.81s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:12 => Train Loss: 4.2357, Train Acc: 0.1341\nEpoch:12 => Val Loss: 5.3708, Val Acc: 0.0123\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:27,  2.50s/it]\u001b[A\n 25%|       | 3/12 [00:04<00:13,  1.49s/it]\u001b[A\n 33%|      | 4/12 [00:04<00:08,  1.02s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.48s/it]\u001b[A\n 50%|     | 6/12 [00:07<00:06,  1.06s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:07,  1.49s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.37s/it]\u001b[A\n 83%| | 10/12 [00:12<00:02,  1.05s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n100%|| 12/12 [00:15<00:00,  1.25s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:19,  3.94s/it]\u001b[A\n 50%|     | 3/6 [00:06<00:05,  1.92s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.31s/it]\u001b[A\n 88%| | 14/16 [05:30<00:45, 22.89s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:13 => Train Loss: 4.0973, Train Acc: 0.1836\nEpoch:13 => Val Loss: 5.4671, Val Acc: 0.0123\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n  8%|         | 1/12 [00:02<00:27,  2.46s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:11,  1.15s/it]\u001b[A\n 25%|       | 3/12 [00:04<00:14,  1.60s/it]\u001b[A\n 33%|      | 4/12 [00:04<00:08,  1.01s/it]\u001b[A\n 42%|     | 5/12 [00:07<00:10,  1.55s/it]\u001b[A\n 58%|    | 7/12 [00:09<00:06,  1.34s/it]\u001b[A\n 67%|   | 8/12 [00:10<00:04,  1.07s/it]\u001b[A\n 75%|  | 9/12 [00:12<00:04,  1.40s/it]\u001b[A\n 83%| | 10/12 [00:12<00:02,  1.07s/it]\u001b[A\n100%|| 12/12 [00:14<00:00,  1.24s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.18s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.68s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.17s/it]\u001b[A\n 94%|| 15/16 [05:52<00:22, 22.64s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:14 => Train Loss: 4.0094, Train Acc: 0.2044\nEpoch:14 => Val Loss: 5.5700, Val Acc: 0.0031\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n  8%|         | 1/12 [00:02<00:28,  2.60s/it]\u001b[A\n 17%|        | 2/12 [00:02<00:11,  1.14s/it]\u001b[A\n 25%|       | 3/12 [00:05<00:18,  2.04s/it]\u001b[A/tmp/ipykernel_28/3158622838.py:15: RuntimeWarning: Mean of empty slice.\n  xyz = xyz - xyz[~np.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n\n 42%|     | 5/12 [00:08<00:10,  1.55s/it]\u001b[A\n 50%|     | 6/12 [00:08<00:07,  1.27s/it]\u001b[A\n 58%|    | 7/12 [00:10<00:07,  1.51s/it]\u001b[A\n 75%|  | 9/12 [00:13<00:04,  1.44s/it]\u001b[A\n 83%| | 10/12 [00:13<00:02,  1.13s/it]\u001b[A\n 92%|| 11/12 [00:15<00:01,  1.40s/it]\u001b[A\n100%|| 12/12 [00:16<00:00,  1.35s/it]\u001b[A\n\n  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n 17%|        | 1/6 [00:03<00:15,  3.08s/it]\u001b[A\n 50%|     | 3/6 [00:05<00:05,  1.70s/it]\u001b[A\n100%|| 6/6 [00:07<00:00,  1.18s/it]\u001b[A\n100%|| 16/16 [06:16<00:00, 23.50s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:15 => Train Loss: 3.9287, Train Acc: 0.2031\nEpoch:15 => Val Loss: 5.6739, Val Acc: 0.0031\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"net_plus.load_state_dict(torch.load('/kaggle/working/00000014.model.pth', map_location= torch.device('cuda'))['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-06-04T20:11:33.694459Z","iopub.execute_input":"2023-06-04T20:11:33.694909Z","iopub.status.idle":"2023-06-04T20:11:33.718008Z","shell.execute_reply.started":"2023-06-04T20:11:33.694873Z","shell.execute_reply":"2023-06-04T20:11:33.717053Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"<p style=\"color:red; font-weight:600; font-size:24px;\">Inference on one example from validation set</p>\n<p style=\"padding-left:60px\">> Modify collate function, or take just one example from the dataset and set up inference</p>","metadata":{}},{"cell_type":"code","source":"def inf_null_collate(batch):\n    batch_size = len(batch)\n    d = {}\n    key = batch.keys()\n    for k in key:\n        d[k] = [b[k] for b in batch]\n    d['label'] = torch.LongTensor(d['label'])\n    return d\n\nfor t, batch in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n        print(batch)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ft_run_inference_on_random_example():\n    kaggle_df = pd.read_csv('/kaggle/input/include-dataset/train-preprocessed.csv')\n    randno = np.random.randint(len(kaggle_df))\n    valid_df = kaggle_df[kaggle_df.index==randno].reset_index(drop=True)\n    valid_dataset = IncludeDataset(valid_df,)\n    print(\"Actual label: \",valid_df.iloc[0]['label'])\n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler = SequentialSampler(valid_dataset),\n        batch_size  = 1,\n        drop_last   = False,\n        num_workers = 0,\n        pin_memory  = False,\n        collate_fn = null_collate,\n    )\n    \n    for t, batch in enumerate(valid_loader):\n        net_plus.output_type = ['inference']\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled = True):\n                output = net_plus(batch)\n                top_values, top_indices = torch.topk(output['sign'].detach().cpu(), k=5)\n                print(top_indices)\n                print(top_values)\n\nif __name__==\"__main__\":\n    ft_run_inference_on_random_example()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}